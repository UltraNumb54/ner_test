import pandas as pd
import re
import json
import argparse

def tokenize_text(text):
    """
    Токенизация текста на слова и знаки препинания
    """
    if pd.isna(text) or text == "":
        return []
    
    # Регулярное выражение для разделения на слова и знаки препинания
    # Учитываем кириллические символы, цифры и основные знаки препинания
    tokens = re.findall(r'[\w\u0400-\u04FF]+|[^\w\s]', str(text))
    return [token for token in tokens if token.strip()]

def process_csv(input_file, output_file):
    """
    Обработка CSV файла: токенизация текста и создание разметки
    """
    try:
        # Чтение CSV файла
        df = pd.read_csv(input_file)
        
        print(f"Загружено {len(df)} строк")
        
        # Обработка каждой строки
        for index, row in df.iterrows():
            # Если success = False или отсутствуют токены
            if row.get('success') == False or not row.get('tokens') or not row.get('ner_tags'):
                original_text = row.get('original_text', '')
                
                if original_text and pd.notna(original_text):
                    # Токенизация текста
                    tokens = tokenize_text(original_text)
                    
                    if tokens:
                        # Создание меток (все 'O')
                        ner_tags = ['O'] * len(tokens)
                        
                        # Обновление строки
                        df.at[index, 'tokens'] = json.dumps(tokens, ensure_ascii=False)
                        df.at[index, 'ner_tags'] = json.dumps(ner_tags, ensure_ascii=False)
                        df.at[index, 'success'] = True
                        
                        print(f"Обработана строка {index}: создано {len(tokens)} токенов")
                    else:
                        print(f"Пропущена строка {index}: не удалось токенизировать текст")
                else:
                    print(f"Пропущена строка {index}: отсутствует original_text")
        
        # Сохранение результата
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"Файл сохранен как: {output_file}")
        print(f"Обработано всего строк: {len(df)}")
        
    except Exception as e:
        print(f"Ошибка при обработке файла: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Обработка CSV файла для NER разметки')
    parser.add_argument('input_file', help='Входной CSV файл')
    parser.add_argument('output_file', help='Выходной CSV файл')
    
    args = parser.parse_args()
    
    process_csv(args.input_file, args.output_file)


python preprocess_csv.py input.csv output.csv

