# Базовая конвертация
python xml_to_ner_converter.py data.xml

# Конвертация с указанием выходного файла
python xml_to_ner_converter.py data.xml -o my_dataset.json

# Конвертация в JSONL формат
python xml_to_ner_converter.py data.xml -o dataset.jsonl --jsonl

# С увеличенным размером чанков
python xml_to_ner_converter.py data.xml --max-length 2000


# xml_to_ner_converter.py
import xml.etree.ElementTree as ET
import json
import re
import html
from typing import List, Dict, Any
import argparse
import os

class XMLToNERConverter:
    def __init__(self, max_chunk_length: int = 1000):
        self.max_chunk_length = max_chunk_length
        
    def clean_text(self, text: str) -> str:
        """
        Очистка текста от проблемных символов и нормализация
        """
        if not text:
            return ""
            
        # Декодируем HTML entities
        text = html.unescape(text)
        
        # Заменяем проблемные кавычки
        text = text.replace('"', "'")
        
        # Удаляем управляющие символы и непечатаемые символы
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        
        # Заменяем множественные пробелы и переносы
        text = re.sub(r'\s+', ' ', text)
        
        # Удаляем начальные и конечные пробелы
        text = text.strip()
        
        return text
    
    def extract_text_from_description(self, description_element) -> str:
        """
        Извлечение текста из description с учетом CDATA
        """
        if description_element is None:
            return ""
            
        text_parts = []
        
        # Рекурсивно обходим все дочерние элементы и текстовые узлы
        for node in description_element.iter():
            if node.text:
                text_parts.append(node.text)
            if node.tail and node.tail.strip():
                text_parts.append(node.tail)
        
        # Если не нашли текста в дочерних элементах, берем напрямую
        if not text_parts and description_element.text:
            text_parts.append(description_element.text)
            
        text = ' '.join(text_parts)
        return self.clean_text(text)
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """
        Разбивка длинного текста на логические части
        """
        if len(text) <= self.max_chunk_length:
            return [text]
        
        chunks = []
        current_chunk = ""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        for sentence in sentences:
            # Если добавление предложения превышает лимит и текущий чанк не пустой
            if len(current_chunk) + len(sentence) > self.max_chunk_length and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def tokenize_text(self, text: str) -> List[str]:
        """
        Токенизация текста для русского языка
        """
        if not text:
            return []
        
        # Улучшенная токенизация для русского языка
        token_regex = r'[\w\u0400-\u04FF@]+|[^\w\s]|_+'
        tokens = re.findall(token_regex, text)
        
        return [token for token in tokens if token.strip()]
    
    def convert_xml_to_json(self, xml_file_path: str, output_file: str = None) -> List[Dict[str, Any]]:
        """
        Основной метод конвертации XML в JSON
        """
        try:
            tree = ET.parse(xml_file_path)
            root = tree.getroot()
        except ET.ParseError as e:
            raise ValueError(f"Ошибка парсинга XML: {e}")
        
        dataset = []
        record_count = 0
        chunk_count = 0
        
        for record in root.findall('record'):
            record_count += 1
            
            # Извлекаем базовые поля
            record_id = record.findtext('id', '').strip()
            service = record.findtext('service', '').strip()
            topic = record.findtext('topic', '').strip()
            
            # Извлекаем и очищаем текст описания
            description_element = record.find('description')
            original_text = self.extract_text_from_description(description_element)
            
            if not original_text:
                print(f"Предупреждение: запись {record_id} не содержит текста")
                continue
            
            # Разбиваем текст на части если нужно
            text_chunks = self.split_text_into_chunks(original_text)
            
            for chunk_index, chunk_text in enumerate(text_chunks):
                chunk_count += 1
                
                # Токенизируем текст
                tokens = self.tokenize_text(chunk_text)
                
                # Создаем запись для датасета
                dataset_entry = {
                    "id": f"{record_id}_{chunk_index}",
                    "original_text": chunk_text,
                    "tokens": tokens,
                    "ner_tags": ["O"] * len(tokens),
                    "success": False,
                    "metadata": {
                        "original_id": record_id,
                        "service": self.clean_text(service),
                        "topic": self.clean_text(topic),
                        "attachments_count": len(record.findall('attachments')),
                        "chunk_index": chunk_index,
                        "total_chunks": len(text_chunks)
                    }
                }
                
                dataset.append(dataset_entry)
        
        print(f"Обработано записей: {record_count}")
        print(f"Создано чанков: {chunk_count}")
        print(f"Всего элементов в датасете: {len(dataset)}")
        
        # Сохраняем в файл если указан output
        if output_file:
            self.save_to_json(dataset, output_file)
            print(f"Датасет сохранен в: {output_file}")
        
        return dataset
    
    def save_to_json(self, dataset: List[Dict], output_file: str):
        """
        Сохранение датасета в JSON файл
        """
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
    
    def save_to_jsonl(self, dataset: List[Dict], output_file: str):
        """
        Сохранение датасета в JSONL формат (одна строка - один JSON)
        """
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in dataset:
                json.dump(item, f, ensure_ascii=False)
                f.write('\n')

def main():
    parser = argparse.ArgumentParser(description='Конвертер XML в NER JSON формат')
    parser.add_argument('input', help='Путь к входному XML файлу')
    parser.add_argument('-o', '--output', help='Путь к выходному JSON файлу', default='ner_dataset.json')
    parser.add_argument('--max-length', type=int, default=1000, help='Максимальная длина текстового чанка')
    parser.add_argument('--jsonl', action='store_true', help='Сохранить в JSONL формат')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"Ошибка: файл {args.input} не найден")
        return
    
    converter = XMLToNERConverter(max_chunk_length=args.max_length)
    
    try:
        dataset = converter.convert_xml_to_json(args.input, None)
        
        if args.jsonl:
            output_file = args.output.replace('.json', '.jsonl') if args.output.endswith('.json') else args.output + '.jsonl'
            converter.save_to_jsonl(dataset, output_file)
        else:
            converter.save_to_json(dataset, args.output)
            
        print("Конвертация завершена успешно!")
        
    except Exception as e:
        print(f"Ошибка при конвертации: {e}")

if __name__ == "__main__":
    main()
