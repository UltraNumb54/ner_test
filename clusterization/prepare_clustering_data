# preprocess_with_ner_model.py
import json
import os
import re
import pandas as pd
from collections import defaultdict
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification

class NERReplacer:
    def __init__(self, model_path):
        """
        Инициализация NER модели для замены сущностей
        """
        print(f"Загрузка NER модели из {model_path}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()
        
        # Загрузка маппинга меток
        with open(f'{model_path}/label_mapping.json', 'r', encoding='utf-8') as f:
            label_mapping = json.load(f)
            self.id2label = label_mapping['id2label']
        
        # Словарь для замены английских меток на русские
        self.metka_translations = {
            'B-FIO': '[ФИО]', 'I-FIO': '[ФИО]',
            'B-PHONE_NUM': '[Телефон]', 'I-PHONE_NUM': '[Телефон]',
            'B-EMAIL': '[Email]', 'I-EMAIL': '[Email]',
            'B-DATE': '[Дата]', 'I-DATE': '[Дата]',
            'B-SUM': '[Сумма]', 'I-SUM': '[Сумма]',
            'B-ORG': '[Организация]', 'I-ORG': '[Организация]',
            'B-LOC': '[Адрес]', 'I-LOC': '[Адрес]',
            'B-READINGS': '[Показания]', 'I-READINGS': '[Показания]',
            'B-LK_NUM': '[Номер лицевого счёта]', 'I-LK_NUM': '[Номер лицевого счёта]'
        }
        
        print("NER модель загружена успешно!")
    
    def predict_entities(self, text):
        """
        Предсказание сущностей в тексте с помощью NER модели
        """
        if not text or not text.strip():
            return []
        
        # Токенизация текста на слова
        words = self.smart_tokenize(text)
        
        if not words:
            return []
        
        # Токенизация для модели
        encoding = self.tokenizer(
            words,
            is_split_into_words=True,
            padding=True,
            truncation=True,
            max_length=256,
            return_tensors="pt",
            return_offsets_mapping=True
        )
        
        # Предсказание
        with torch.no_grad():
            inputs = {k: v.to(self.device) for k, v in encoding.items()}
            outputs = self.model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=2)
        
        # Обработка результатов
        entities = self._process_predictions(words, encoding, predictions)
        return entities
    
    def smart_tokenize(self, text):
        """
        Умная токенизация текста
        """
        if not text or not text.strip():
            return []
        
        tokens = []
        current_token = ""
        
        for char in text:
            if char.isalnum() or char in ['-', '_', '@', '.', '+', '/', '\\']:
                current_token += char
            else:
                if current_token:
                    tokens.append(current_token)
                    current_token = ""
                if char.strip():
                    tokens.append(char)
        
        if current_token:
            tokens.append(current_token)
            
        return tokens
    
    def _process_predictions(self, original_tokens, encoding, predictions):
        """
        Обработка предсказаний модели
        """
        entities = []
        word_ids = encoding.word_ids(batch_index=0)
        
        current_entity = ""
        current_entity_type = None
        current_entity_start = None
        
        for i, (word_id, pred_id) in enumerate(zip(word_ids, predictions[0])):
            if word_id is None:
                continue
            
            if encoding['input_ids'][0][i] == self.tokenizer.pad_token_id:
                continue
            
            token = original_tokens[word_id] if word_id < len(original_tokens) else ""
            pred_str = str(pred_id.item())
            label = self.id2label.get(pred_str, 'O')
            
            # Если это начало новой сущности
            if label.startswith('B-') or (label != 'O' and current_entity_type != self.metka_translations.get(label, label)):
                # Сохраняем предыдущую сущность если есть
                if current_entity:
                    entities.append({
                        'text': current_entity.strip(),
                        'type': current_entity_type,
                        'start_pos': current_entity_start,
                        'end_pos': i
                    })
                
                # Начинаем новую сущность
                current_entity = token
                current_entity_type = self.metka_translations.get(label, label)
                current_entity_start = i
            
            # Если продолжаем текущую сущность
            elif label != 'O' and current_entity_type == self.metka_translations.get(label, label):
                # Добавляем к текущей сущности
                if token.strip() and not token.startswith(('.', ',', '!', '?', ':', ';')):
                    current_entity += " " + token
                else:
                    current_entity += token
            
            # Если сущность закончилась
            elif label == 'O' and current_entity:
                entities.append({
                    'text': current_entity.strip(),
                    'type': current_entity_type,
                    'start_pos': current_entity_start,
                    'end_pos': i
                })
                current_entity = ""
                current_entity_type = None
        
        # Добавляем последнюю сущность если есть
        if current_entity:
            entities.append({
                'text': current_entity.strip(),
                'type': current_entity_type,
                'start_pos': current_entity_start,
                'end_pos': len(word_ids)
            })
        
        return entities
    
    def replace_entities_in_text(self, text):
        """
        Замена сущностей в тексте на русские метки с объединением подряд идущих
        """
        entities = self.predict_entities(text)
        
        if not entities:
            return text, []
        
        # Сортируем сущности по позиции начала
        entities.sort(key=lambda x: x['start_pos'])
        
        # Заменяем сущности в тексте
        result_text = text
        replacements = []
        offset = 0
        
        for entity in entities:
            original_text = entity['text']
            replacement = entity['type']
            
            # Находим позицию в тексте с учетом предыдущих замен
            pos = result_text.find(original_text, offset)
            if pos != -1:
                # Заменяем текст
                result_text = result_text[:pos] + replacement + result_text[pos + len(original_text):]
                
                replacements.append({
                    'original': original_text,
                    'replacement': replacement,
                    'position': pos
                })
                
                # Обновляем offset для следующего поиска
                offset = pos + len(replacement)
        
        # Объединяем подряд идущие одинаковые метки
        result_text = self.merge_consecutive_metkas(result_text)
        
        return result_text, replacements
    
    def merge_consecutive_metkas(self, text):
        """
        Объединяет подряд идущие одинаковые метки в одну
        """
        # Все возможные метки
        metkas = [
            '[ФИО]', '[Телефон]', '[Email]', '[Дата]', '[Сумма]', 
            '[Организация]', '[Адрес]', '[Показания]', '[Номер лицевого счёта]'
        ]
        
        for metka in metkas:
            # Регулярное выражение для поиска 2+ подряд идущих одинаковых меток
            pattern = f'({re.escape(metka)}\\s*)+'
            # Заменяем на одну метку
            text = re.sub(pattern, metka + ' ', text)
        
        # Убираем лишние пробелы
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

def create_three_files_with_ner(input_file, output_analysis, output_clustering, output_first_messages, model_path):
    """
    Создает три файла с использованием NER модели для замен сущностей
    """
    print(f"Чтение файла: {input_file}")
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"Обработка {len(data)} диалогов...")
    
    # Инициализация NER заменщика
    ner_replacer = NERReplacer(model_path)
    
    # Списки для трех файлов
    analysis_data = []      # Для анализа: текст до/после замен
    clustering_data = []    # Для кластеризации: только текст с заменами
    first_messages_data = [] # Только первые сообщения (без замен)
    
    for dialog in data:
        dialog_id = dialog.get('id', 'unknown')
        topic = dialog.get('topic', 'unknown')
        source = dialog.get('source', 'unknown')
        
        # Собираем все сообщения диалога
        all_messages = []
        all_texts_original = []
        all_texts_replaced = []
        all_replacements = []
        
        if 'dialogue' in dialog:
            for message in dialog['dialogue']:
                if 'text' in message and message['text'].strip():
                    text = message['text'].strip()
                    all_texts_original.append(text)
                    
                    # Применяем замены с помощью NER модели
                    replaced_text, replacements = ner_replacer.replace_entities_in_text(text)
                    all_texts_replaced.append(replaced_text)
                    all_replacements.extend(replacements)
                    
                    all_messages.append({
                        'role': message.get('role', 'unknown'),
                        'text_original': text,
                        'text_replaced': replaced_text
                    })
        
        # 1. Файл для анализа
        if all_texts_original:
            full_text_original = ' '.join(all_texts_original)
            full_text_replaced = ' '.join(all_texts_replaced)
            
            analysis_data.append({
                'dialog_id': dialog_id,
                'topic': topic,
                'source': source,
                'messages': all_messages,
                'replacements': all_replacements,
                'full_text_original': full_text_original,
                'full_text_replaced': full_text_replaced
            })
        
        # 2. Файл для кластеризации
        if all_texts_replaced:
            clustering_data.append({
                'dialog_id': dialog_id,
                'topic': topic,
                'source': source,
                'full_text': ' '.join(all_texts_replaced),
                'text_length': len(' '.join(all_texts_replaced))
            })
        
        # 3. Файл с первыми сообщениями (БЕЗ ЗАМЕН)
        if all_texts_original:
            first_messages_data.append({
                'dialog_id': dialog_id,
                'topic': topic,
                'source': source,
                'first_message': all_texts_original[0],  # Исходный текст без замен
                'first_message_length': len(all_texts_original[0])
            })
    
    # Сохраняем файлы
    print("\nСохранение файлов...")
    
    # 1. Файл анализа
    with open(output_analysis, 'w', encoding='utf-8') as f:
        json.dump(analysis_data, f, ensure_ascii=False, indent=2)
    print(f"Файл анализа сохранен: {output_analysis}")
    
    # 2. Файл для кластеризации
    with open(output_clustering, 'w', encoding='utf-8') as f:
        json.dump(clustering_data, f, ensure_ascii=False, indent=2)
    print(f"Файл для кластеризации сохранен: {output_clustering}")
    
    # 3. Файл с первыми сообщениями
    with open(output_first_messages, 'w', encoding='utf-8') as f:
        json.dump(first_messages_data, f, ensure_ascii=False, indent=2)
    print(f"Файл с первыми сообщениями сохранен: {output_first_messages}")
    
    # Статистика
    print("\nСтатистика:")
    print(f"  Всего диалогов: {len(data)}")
    print(f"  Диалогов в анализе: {len(analysis_data)}")
    print(f"  Диалогов для кластеризации: {len(clustering_data)}")
    print(f"  Первых сообщений: {len(first_messages_data)}")
    
    # Анализ замен
    if analysis_data:
        all_replacements_count = sum(len(dialog['replacements']) for dialog in analysis_data)
        print(f"  Всего замен: {all_replacements_count}")
        
        # Статистика по типам замен
        replacement_stats = defaultdict(int)
        for dialog in analysis_data:
            for replacement in dialog['replacements']:
                replacement_stats[replacement['replacement']] += 1
        
        print("  Распределение замен:")
        for replacement_type, count in sorted(replacement_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"    {replacement_type}: {count}")
    
    return analysis_data, clustering_data, first_messages_data

def test_ner_replacements(ner_replacer):
    """
    Тестирование замен с помощью NER модели
    """
    test_cases = [
        "Меня зовут Иван Иванов, телефон +79161234567",
        "Стоимость 1500 рублей, дата 15.12.2023",
        "Email: test@example.com, номер счета 1234567890",
        "ООО Ромашка, город Москва, улица Ленина дом 5",
        "Показания 125.5 кВт, сумма 2500.75 руб"
    ]
    
    print("\nТестирование NER замен:")
    print("=" * 50)
    
    for i, test in enumerate(test_cases, 1):
        replaced, replacements = ner_replacer.replace_entities_in_text(test)
        print(f"Пример {i}:")
        print(f"  Оригинал: '{test}'")
        print(f"  Заменено: '{replaced}'")
        if replacements:
            print(f"  Замены: {replacements}")
        print()

def main():
    """
    Основная функция
    """
    # ==================== КОНФИГУРАЦИЯ ====================
    INPUT_FILE = "input_data.json"                  # Исходный JSON файл
    OUTPUT_ANALYSIS = "analysis_data_ner.json"      # Файл для анализа
    OUTPUT_CLUSTERING = "clustering_data_ner.json"  # Файл для кластеризации  
    OUTPUT_FIRST_MESSAGES = "first_messages.json"   # Файл с первыми сообщениями (без замен)
    NER_MODEL_PATH = "ner-final-model"              # Путь к дообученной NER модели
    TEST_NER = True                                # Протестировать NER замены
    # ======================================================
    
    print("=" * 60)
    print("ПРЕДОБРАБОТКА С ИСПОЛЬЗОВАНИЕМ NER МОДЕЛИ")
    print("=" * 60)
    
    if not os.path.exists(INPUT_FILE):
        print(f"Ошибка: файл {INPUT_FILE} не найден!")
        return
    
    if not os.path.exists(NER_MODEL_PATH):
        print(f"Ошибка: NER модель {NER_MODEL_PATH} не найдена!")
        return
    
    try:
        # Тестирование NER замен
        if TEST_NER:
            ner_replacer = NERReplacer(NER_MODEL_PATH)
            test_ner_replacements(ner_replacer)
        
        # Создание трех файлов
        analysis, clustering, first_messages = create_three_files_with_ner(
            INPUT_FILE, 
            OUTPUT_ANALYSIS, 
            OUTPUT_CLUSTERING, 
            OUTPUT_FIRST_MESSAGES,
            NER_MODEL_PATH
        )
        
        print("\n" + "=" * 60)
        print("ПРЕДОБРАБОТКА ЗАВЕРШЕНА!")
        print("=" * 60)
        
    except Exception as e:
        print(f"Ошибка при обработке: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
