# preprocess_entities_all_replaced.py
import json
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
import re
from tqdm import tqdm
import os
from collections import defaultdict

class EntityReplacer:
    def __init__(self, model_path):
        """Инициализация NER модели для замены сущностей"""
        print(f"Загрузка NER модели из {model_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Используется устройство: {self.device}")
        self.model.to(self.device)
        self.model.eval()
        
        # Загрузка маппинга меток
        try:
            with open(f'{model_path}/label_mapping.json', 'r', encoding='utf-8') as f:
                label_mapping = json.load(f)
                self.id2label = {str(k): v for k, v in label_mapping['id2label'].items()}
            print("Загружен пользовательский маппинг меток")
        except:
            # Используем стандартный маппинг модели
            self.id2label = {str(k): v for k, v in self.model.config.id2label.items()}
            print("Используется стандартный маппинг меток модели")
        
        # Словарь для замены английских меток на русские
        self.metka_translations = {
            'B-FIO': '[ФИО]', 'I-FIO': '[ФИО]',
            'B-PHONE_NUM': '[Телефон]', 'I-PHONE_NUM': '[Телефон]',
            'B-EMAIL': '[Email]', 'I-EMAIL': '[Email]',
            'B-DATE': '[Дата]', 'I-DATE': '[Дата]',
            'B-SUM': '[Сумма]', 'I-SUM': '[Сумма]',
            'B-ORG': '[Организация]', 'I-ORG': '[Организация]',
            'B-LOC': '[Адрес]', 'I-LOC': '[Адрес]',
            'B-READINGS': '[Показания]', 'I-READINGS': '[Показания]',
            'B-LK_NUM': '[Номер лицевого счёта]', 'I-LK_NUM': '[Номер лицевого счёта]'
        }
    
    def clean_text(self, text):
        """Очистка текста от лишних пробелов"""
        if not text:
            return ""
        text = re.sub(r' +', ' ', text)
        return text.strip()
    
    def merge_consecutive_metkas(self, text):
        """Объединяет подряд идущие одинаковые метки"""
        metkas = ['[ФИО]', '[Телефон]', '[Email]', '[Дата]', '[Сумма]', 
                 '[Организация]', '[Адрес]', '[Показания]', '[Номер лицевого счёта]']
        
        for metka in metkas:
            pattern = f'({re.escape(metka)}\\s*)+'
            text = re.sub(pattern, metka + ' ', text)
        
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def replace_entities_in_text(self, text):
        """Замена сущностей на метки в тексте с сохранением пробелов"""
        if not text or len(text.strip()) == 0:
            return text, []
        
        try:
            # Очищаем текст перед обработкой
            text = self.clean_text(text)
            
            # Токенизация текста
            inputs = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt",
                return_offsets_mapping=True
            )
            
            # Извлекаем смещения
            offset_mapping = inputs['offset_mapping']
            
            # Убираем offset_mapping из inputs для модели
            inputs_for_model = {k: v for k, v in inputs.items() if k != 'offset_mapping'}
            inputs_for_model = {k: v.to(self.device) for k, v in inputs_for_model.items()}
            
            # Предсказание
            with torch.no_grad():
                outputs = self.model(**inputs_for_model)
                predictions = torch.argmax(outputs.logits, dim=2)
            
            # Получаем предсказания для первого примера в батче
            predictions = predictions[0].cpu().numpy()
            offsets = offset_mapping[0].cpu().numpy()
            
            # Собираем сущности
            entities = []
            current_entity = None
            
            for i, (pred, (start, end)) in enumerate(zip(predictions, offsets)):
                # Пропускаем специальные токены и padding
                if start == 0 and end == 0:
                    continue
                
                pred_int = int(pred)
                label = self.id2label.get(str(pred_int), 'O')
                
                # Переводим метку на русский
                translated_label = self.metka_translations.get(label, 'O')
                
                if label.startswith('B-'):
                    # Начало новой сущности
                    if current_entity is not None:
                        entities.append(current_entity)
                    current_entity = {
                        'start': start,
                        'end': end,
                        'type': translated_label,
                        'text': text[start:end]
                    }
                elif label.startswith('I-') and current_entity is not None and current_entity['type'] == translated_label:
                    # Продолжение сущности
                    current_entity['end'] = end
                    current_entity['text'] = text[current_entity['start']:end]
                elif label == 'O' and current_entity is not None:
                    # Конец сущности
                    entities.append(current_entity)
                    current_entity = None
            
            # Добавляем последнюю сущность если есть
            if current_entity is not None:
                entities.append(current_entity)
            
            # Заменяем сущности в тексте (с конца чтобы не сбивались индексы)
            result_text = text
            replacements = []
            
            for entity in sorted(entities, key=lambda x: x['start'], reverse=True):
                original_text = entity['text']
                replacement = entity['type']
                
                # Заменяем в тексте
                result_text = result_text[:entity['start']] + replacement + result_text[entity['end']:]
                
                replacements.append({
                    'original': original_text,
                    'replacement': replacement,
                    'start': entity['start'],
                    'end': entity['end']
                })
            
            # Объединяем подряд идущие метки
            result_text = self.merge_consecutive_metkas(result_text)
            result_text = self.clean_text(result_text)
            
            return result_text, replacements
            
        except Exception as e:
            print(f"Ошибка при обработке текста: {e}")
            return text, []

def create_three_files(input_file, model_path, output_analysis, output_clustering, output_first_messages):
    """
    Создает три файла с разными форматами данных
    Все файлы содержат текст с заменами через NER модель
    """
    print(f"Загрузка данных из {input_file}...")
    
    # Проверка существования файлов
    if not os.path.exists(input_file):
        print(f"Ошибка: Файл {input_file} не существует")
        return
    
    if not os.path.exists(model_path):
        print(f"Ошибка: Модель {model_path} не существует")
        return
    
    # Загрузка исходного JSON
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"Ошибка загрузки JSON: {e}")
        return
    
    print(f"Загружено диалогов: {len(data)}")
    
    # Инициализация заменщика сущностей
    replacer = EntityReplacer(model_path)
    
    # Данные для трех файлов
    analysis_data = []      # Для анализа: полная информация о заменах
    clustering_data = []    # Для кластеризации: только текст с заменами
    first_messages_data = [] # Первые сообщения с заменами
    
    # Статистика
    stats = {
        'total_dialogs': len(data),
        'total_messages': 0,
        'processed_messages': 0,
        'total_replacements': 0
    }
    
    for dialog in tqdm(data, desc="Обработка диалогов"):
        dialog_id = dialog.get('id', 'unknown')
        topic = dialog.get('topic', 'unknown')
        source = dialog.get('source', 'unknown')
        
        # Данные для анализа
        dialog_analysis = {
            'dialog_id': dialog_id,
            'topic': topic,
            'source': source,
            'messages': [],
            'total_replacements': 0,
            'all_text_original': '',
            'all_text_replaced': ''
        }
        
        # Данные для кластеризации
        dialog_clustering = {
            'dialog_id': dialog_id,
            'topic': topic,
            'source': source,
            'text': '',
            'text_length': 0
        }
        
        # Данные для первых сообщений (ТЕПЕРЬ ТОЖЕ С ЗАМЕНАМИ)
        dialog_first_message = {
            'dialog_id': dialog_id,
            'topic': topic,
            'source': source,
            'first_message_original': '',  # Оригинальное первое сообщение
            'first_message_replaced': '',  # Первое сообщение с заменами
            'first_message_length': 0
        }
        
        # Обработка каждого сообщения в диалоге
        all_messages_original = []
        all_messages_replaced = []
        
        for i, message in enumerate(dialog.get('dialogue', [])):
            stats['total_messages'] += 1
            
            original_text = message.get('text', '')
            role = message.get('role', 'unknown')
            
            if not original_text.strip():
                continue
            
            # Очищаем текст
            cleaned_text = original_text.replace('\n', ' ').replace('\r', ' ').strip()
            
            # Замена сущностей во ВСЕХ сообщениях
            replaced_text, replacements = replacer.replace_entities_in_text(cleaned_text)
            
            # Для первого сообщения сохраняем и оригинал и замененный вариант
            if i == 0:
                dialog_first_message['first_message_original'] = cleaned_text
                dialog_first_message['first_message_replaced'] = replaced_text
                dialog_first_message['first_message_length'] = len(replaced_text)
            
            # Собираем статистику
            if replacements:
                stats['processed_messages'] += 1
                stats['total_replacements'] += len(replacements)
                dialog_analysis['total_replacements'] += len(replacements)
            
            # Сохраняем для анализа
            message_analysis = {
                'role': role,
                'text_original': cleaned_text,
                'text_replaced': replaced_text,
                'replacements': replacements
            }
            dialog_analysis['messages'].append(message_analysis)
            
            # Сохраняем тексты для объединения
            all_messages_original.append(cleaned_text)
            all_messages_replaced.append(replaced_text)
        
        # Формируем полные тексты для анализа и кластеризации
        full_text_original = " ".join(all_messages_original)
        full_text_replaced = " ".join(all_messages_replaced)
        
        dialog_analysis['all_text_original'] = full_text_original
        dialog_analysis['all_text_replaced'] = full_text_replaced
        
        dialog_clustering['text'] = full_text_replaced
        dialog_clustering['text_length'] = len(full_text_replaced)
        
        # Добавляем в соответствующие списки
        analysis_data.append(dialog_analysis)
        clustering_data.append(dialog_clustering)
        
        # Добавляем first_message только если есть первое сообщение
        if dialog_first_message['first_message_replaced']:
            first_messages_data.append(dialog_first_message)
    
    # Сохранение трех файлов
    print("\nСохранение файлов...")
    
    try:
        # 1. Файл анализа
        with open(output_analysis, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)
        print(f"Файл анализа сохранен: {output_analysis}")
        
        # 2. Файл для кластеризации
        with open(output_clustering, 'w', encoding='utf-8') as f:
            json.dump(clustering_data, f, ensure_ascii=False, indent=2)
        print(f"Файл для кластеризации сохранен: {output_clustering}")
        
        # 3. Файл с первыми сообщениями (ТЕПЕРЬ С ЗАМЕНАМИ)
        with open(output_first_messages, 'w', encoding='utf-8') as f:
            json.dump(first_messages_data, f, ensure_ascii=False, indent=2)
        print(f"Файл с первыми сообщениями сохранен: {output_first_messages}")
        
        # Статистика
        print(f"\nСтатистика обработки:")
        print(f"  Всего диалогов: {stats['total_dialogs']}")
        print(f"  Всего сообщений: {stats['total_messages']}")
        print(f"  Сообщений с заменами: {stats['processed_messages']}")
        print(f"  Всего замен: {stats['total_replacements']}")
        print(f"  Файлов создано: 3")
        
        # Статистика по типам замен
        replacement_stats = defaultdict(int)
        for dialog in analysis_data:
            for message in dialog['messages']:
                for replacement in message['replacements']:
                    replacement_stats[replacement['replacement']] += 1
        
        print(f"\nРаспределение типов замен:")
        for metka_type, count in sorted(replacement_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"  {metka_type}: {count}")
        
    except Exception as e:
        print(f"Ошибка сохранения файлов: {e}")
    
    return analysis_data, clustering_data, first_messages_data

def main():
    """Основная функция"""
    
    # ==================== КОНФИГУРАЦИЯ ====================
    INPUT_FILE = "input_data.json"                  # Исходный JSON файл с диалогами
    MODEL_PATH = "ner-final-model"                  # Путь к NER модели
    OUTPUT_ANALYSIS = "analysis_data.json"          # Файл для анализа
    OUTPUT_CLUSTERING = "clustering_data.json"      # Файл для кластеризации
    OUTPUT_FIRST_MESSAGES = "first_messages.json"   # Файл с первыми сообщениями (теперь с заменами)
    # ======================================================
    
    print("=" * 60)
    print("СОЗДАНИЕ ТРЕХ ФАЙЛОВ ДЛЯ АНАЛИЗА И КЛАСТЕРИЗАЦИИ")
    print("=" * 60)
    print(f"Входной файл: {INPUT_FILE}")
    print(f"Модель: {MODEL_PATH}")
    print(f"Выходные файлы:")
    print(f"  Анализ: {OUTPUT_ANALYSIS}")
    print(f"  Кластеризация: {OUTPUT_CLUSTERING}")
    print(f"  Первые сообщения: {OUTPUT_FIRST_MESSAGES} (с заменами)")
    print("=" * 60)
    
    # Создаем папки для выходных файлов если не существуют
    for output_file in [OUTPUT_ANALYSIS, OUTPUT_CLUSTERING, OUTPUT_FIRST_MESSAGES]:
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)
    
    # Запуск обработки
    analysis, clustering, first_messages = create_three_files(
        INPUT_FILE, 
        MODEL_PATH, 
        OUTPUT_ANALYSIS, 
        OUTPUT_CLUSTERING, 
        OUTPUT_FIRST_MESSAGES
    )
    
    print("\n" + "=" * 60)
    print("ОБРАБОТКА ЗАВЕРШЕНА!")
    print("=" * 60)

if __name__ == "__main__":
    main()
