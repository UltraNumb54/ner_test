# advanced_topic_clustering.py
import json
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import argparse
from tqdm import tqdm

class AdvancedTopicClustering:
    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):
        """Инициализация модели для создания эмбеддингов"""
        print(f"Загрузка модели {model_name}...")
        self.model = SentenceTransformer(model_name)
        print("Модель загружена успешно!")
    
    def preprocess_texts(self, json_file):
        """Подготовка текстов для кластеризации"""
        print("Чтение JSON файла...")
        
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        dialog_texts = []
        dialog_info = []
        
        for dialog in data:
            # Объединяем все сообщения диалога в один текст
            full_text = " ".join([message['text'] for message in dialog['dialogue']])
            
            # Очищаем текст от лишних пробелов и коротких текстов
            clean_text = re.sub(r'\s+', ' ', full_text).strip()
            
            if len(clean_text) > 20:  # Пропускаем слишком короткие тексты
                dialog_texts.append(clean_text)
                dialog_info.append({
                    'dialog_id': dialog['id'],
                    'original_topic': dialog.get('topic', 'unknown'),
                    'source': dialog.get('source', 'unknown'),
                    'timestamp': dialog.get('timestamp', ''),
                    'text_length': len(clean_text)
                })
        
        print(f"Подготовлено {len(dialog_texts)} диалогов для кластеризации")
        return dialog_texts, dialog_info
    
    def create_embeddings(self, texts, batch_size=32):
        """Создание векторных представлений текстов"""
        print("Создание эмбеддингов...")
        
        embeddings = self.model.encode(
            texts, 
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        print(f"Создано эмбеддингов: {embeddings.shape}")
        return embeddings
    
    def find_optimal_clusters(self, embeddings, max_k=15):
        """Поиск оптимального количества кластеров методом локтя"""
        print("Поиск оптимального количества кластеров...")
        
        wcss = []  # Within-Cluster Sum of Squares
        silhouette_scores = []
        k_range = range(2, min(max_k + 1, len(embeddings)))
        
        for k in tqdm(k_range, desc="Анализ кластеров"):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(embeddings)
            wcss.append(kmeans.inertia_)
            
            if len(set(labels)) > 1:
                score = silhouette_score(embeddings, labels)
                silhouette_scores.append(score)
            else:
                silhouette_scores.append(0)
        
        # Находим оптимальное k по методу локтя
        differences = []
        for i in range(1, len(wcss)):
            differences.append(wcss[i-1] - wcss[i])
        
        # Ищем "локоть" - точку, где разница начинает значительно уменьшаться
        optimal_k_elbow = 2
        for i in range(1, len(differences)-1):
            if differences[i] < differences[i-1] * 0.7:  # Эвристика для нахождения "локтя"
                optimal_k_elbow = k_range[i]
                break
        
        # Находим оптимальное k по silhouette score
        optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]
        
        print(f"Оптимальное количество кластеров (метод локтя): {optimal_k_elbow}")
        print(f"Оптимальное количество кластеров (silhouette): {optimal_k_silhouette}")
        
        # Визуализация
        self._plot_cluster_analysis(k_range, wcss, silhouette_scores, 
                                  optimal_k_elbow, optimal_k_silhouette)
        
        # Возвращаем среднее значение, округленное в большую сторону
        optimal_k = max(optimal_k_elbow, optimal_k_silhouette)
        return optimal_k
    
    def _plot_cluster_analysis(self, k_range, wcss, silhouette_scores, k_elbow, k_silhouette):
        """Визуализация анализа кластеров"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Метод локтя
        ax1.plot(k_range, wcss, 'bo-')
        ax1.axvline(x=k_elbow, color='red', linestyle='--', alpha=0.7)
        ax1.set_xlabel('Количество кластеров')
        ax1.set_ylabel('WCSS')
        ax1.set_title('Метод локтя')
        ax1.grid(True, alpha=0.3)
        
        # Silhouette score
        ax2.plot(k_range, silhouette_scores, 'go-')
        ax2.axvline(x=k_silhouette, color='red', linestyle='--', alpha=0.7)
        ax2.set_xlabel('Количество кластеров')
        ax2.set_ylabel('Silhouette Score')
        ax2.set_title('Silhouette Analysis')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('cluster_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("График анализа кластеров сохранен как 'cluster_analysis.png'")
    
    def perform_clustering(self, embeddings, n_clusters=None):
        """Выполнение кластеризации K-means"""
        if n_clusters is None:
            n_clusters = min(10, max(2, len(embeddings) // 10))
        
        print(f"Выполнение K-means кластеризации с {n_clusters} кластерами...")
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings)
        
        # Оценка качества кластеризации
        if len(set(labels)) > 1:
            score = silhouette_score(embeddings, labels)
            print(f"Silhouette Score: {score:.4f}")
        else:
            print("Создан только один кластер")
        
        return labels, kmeans
    
    def extract_cluster_keywords(self, texts, labels, top_n=10):
        """Извлечение ключевых слов для каждого кластера"""
        print("Извлечение ключевых слов кластеров...")
        
        cluster_keywords = {}
        
        for cluster_id in set(labels):
            cluster_texts = [text for text, label in zip(texts, labels) if label == cluster_id]
            
            if cluster_texts:
                # Объединяем все тексты кластера
                all_text = " ".join(cluster_texts)
                
                # Извлекаем слова (игнорируем стоп-слова и короткие слова)
                words = re.findall(r'\b[а-яё]{4,}\b', all_text.lower())
                
                # Считаем частотность
                word_counts = Counter(words)
                
                # Исключаем стоп-слова
                stop_words = {'этот', 'такой', 'какой', 'который', 'очень', 'много', 'можно', 'нужно', 
                             'должен', 'хочу', 'хотеть', 'хотят', 'свой', 'мочь', 'хотеть', 'говорить',
                             'сказать', 'дело', 'время', 'человек', 'работа', 'компания', 'город'}
                
                keywords = [(word, count) for word, count in word_counts.most_common(50) 
                           if word not in stop_words and count > 1]
                
                cluster_keywords[cluster_id] = keywords[:top_n]
        
        return cluster_keywords
    
    def assign_topic_names(self, cluster_keywords):
        """Присвоение понятных названий темам"""
        topic_names = {}
        
        for cluster_id, keywords in cluster_keywords.items():
            if keywords:
                # Берем топ-2 ключевых слова для названия
                top_keywords = [word for word, count in keywords[:2]]
                topic_name = f"Тема_{cluster_id}_{'_'.join(top_keywords)}"
            else:
                topic_name = f"Тема_{cluster_id}"
            
            topic_names[cluster_id] = {
                'name': topic_name,
                'keywords': [word for word, count in keywords[:5]],
                'keyword_scores': {word: count for word, count in keywords[:5]}
            }
        
        return topic_names
    
    def visualize_clusters(self, embeddings, labels, topic_names, output_file='clusters_visualization.png'):
        """Визуализация кластеров с помощью PCA"""
        print("Визуализация кластеров...")
        
        # Уменьшаем размерность до 2D для визуализации
        pca = PCA(n_components=2, random_state=42)
        embeddings_2d = pca.fit_transform(embeddings)
        
        plt.figure(figsize=(12, 10))
        
        # Создаем датафрейм для удобства
        df = pd.DataFrame({
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1],
            'cluster': labels,
            'topic': [topic_names.get(label, {}).get('name', f'Cluster {label}') for label in labels]
        })
        
        # Рисуем scatter plot
        scatter = plt.scatter(df['x'], df['y'], c=df['cluster'], cmap='tab20', alpha=0.7, s=50)
        
        # Добавляем центроиды
        for cluster_id in set(labels):
            cluster_points = df[df['cluster'] == cluster_id]
            if len(cluster_points) > 0:
                centroid_x = cluster_points['x'].mean()
                centroid_y = cluster_points['y'].mean()
                plt.scatter(centroid_x, centroid_y, marker='x', s=200, linewidths=3, 
                           color='red', alpha=0.8)
                
                # Подписываем центроиды
                topic_name = topic_names.get(cluster_id, {}).get('name', f'Cluster {cluster_id}')
                plt.annotate(topic_name, (centroid_x, centroid_y), 
                            xytext=(10, 10), textcoords='offset points',
                            fontsize=9, alpha=0.9,
                            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
        
        plt.colorbar(scatter)
        plt.title('Визуализация кластеров диалогов (PCA)')
        plt.xlabel('Компонента 1')
        plt.ylabel('Компонента 2')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Визуализация кластеров сохранена как '{output_file}'")
    
    def save_results(self, original_json_file, clustered_data, topic_names, output_file):
        """Сохранение результатов кластеризации"""
        print("Сохранение результатов...")
        
        with open(original_json_file, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        
        # Создаем маппинг dialog_id -> topic_info
        topic_mapping = {}
        for item in clustered_data:
            topic_mapping[item['dialog_id']] = {
                'cluster_id': int(item['cluster_id']),
                'topic_name': topic_names[item['cluster_id']]['name'],
                'topic_keywords': topic_names[item['cluster_id']]['keywords'],
                'embedding_dims': 384
            }
        
        # Обновляем исходные данные
        for dialog in original_data:
            dialog_id = dialog['id']
            if dialog_id in topic_mapping:
                dialog['predicted_topic'] = topic_mapping[dialog_id]['topic_name']
                dialog['topic_cluster'] = topic_mapping[dialog_id]['cluster_id']
                dialog['topic_keywords'] = topic_mapping[dialog_id]['topic_keywords']
                dialog['clustering_method'] = 'sentence-transformers + K-means'
            else:
                dialog['predicted_topic'] = 'Не распределен'
                dialog['topic_cluster'] = -1
                dialog['topic_keywords'] = []
        
        # Сохраняем результат
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(original_data, f, ensure_ascii=False, indent=2)
        
        # Статистика
        self._print_statistics(original_data)
        
        return original_data
    
    def _print_statistics(self, data):
        """Вывод статистики кластеризации"""
        topic_stats = {}
        cluster_sizes = {}
        
        for dialog in data:
            topic = dialog['predicted_topic']
            cluster = dialog['topic_cluster']
            
            if topic not in topic_stats:
                topic_stats[topic] = 0
            topic_stats[topic] += 1
            
            if cluster not in cluster_sizes:
                cluster_sizes[cluster] = 0
            cluster_sizes[cluster] += 1
        
        print("\n" + "="*50)
        print("СТАТИСТИКА КЛАСТЕРИЗАЦИИ")
        print("="*50)
        print(f"Всего диалогов: {len(data)}")
        print(f"Количество тем: {len(topic_stats)}")
        
        print("\nРаспределение по темам:")
        for topic, count in sorted(topic_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"  {topic}: {count} диалогов")
        
        print(f"\nРазмеры кластеров:")
        for cluster, size in sorted(cluster_sizes.items()):
            print(f"  Кластер {cluster}: {size} диалогов")

def main():
    """Основная функция"""
    parser = argparse.ArgumentParser(description='Продвинутая кластеризация диалогов по темам')
    parser.add_argument('--input_file', type=str, required=True, help='JSON файл с диалогами')
    parser.add_argument('--output_file', type=str, default='clustered_topics.json', help='Выходной JSON файл')
    parser.add_argument('--num_clusters', type=int, default=None, help='Количество кластеров (автоопределение если не задано)')
    parser.add_argument('--batch_size', type=int, default=32, help='Размер батча для создания эмбеддингов')
    
    args = parser.parse_args()
    
    # Инициализация кластеризатора
    clusterer = AdvancedTopicClustering()
    
    # Подготовка данных
    texts, dialog_info = clusterer.preprocess_texts(args.input_file)
    
    if len(texts) < 3:
        print("Недостаточно данных для кластеризации (нужно минимум 3 диалога)")
        return
    
    # Создание эмбеддингов
    embeddings = clusterer.create_embeddings(texts, args.batch_size)
    
    # Определение оптимального количества кластеров
    if args.num_clusters is None:
        args.num_clusters = clusterer.find_optimal_clusters(embeddings)
    
    # Кластеризация
    labels, kmeans = clusterer.perform_clustering(embeddings, args.num_clusters)
    
    # Анализ кластеров
    cluster_keywords = clusterer.extract_cluster_keywords(texts, labels)
    topic_names = clusterer.assign_topic_names(cluster_keywords)
    
    # Подготовка данных для сохранения
    clustered_data = []
    for text, info, label in zip(texts, dialog_info, labels):
        clustered_data.append({
            **info,
            'text_preview': text[:100] + '...' if len(text) > 100 else text,
            'cluster_id': int(label)
        })
    
    # Визуализация
    clusterer.visualize_clusters(embeddings, labels, topic_names)
    
    # Сохранение результатов
    clusterer.save_results(args.input_file, clustered_data, topic_names, args.output_file)
    
    print(f"\nКластеризация завершена! Результаты сохранены в {args.output_file}")

if __name__ == "__main__":
    main()
