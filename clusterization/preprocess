# replace_entities.py
import json
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
import re
import argparse
from tqdm import tqdm

class EntityReplacer:
    def __init__(self, model_path):
        """Инициализация NER модели для замены сущностей"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()
        
        # Загрузка маппинга меток
        with open(f'{model_path}/label_mapping.json', 'r', encoding='utf-8') as f:
            label_mapping = json.load(f)
            self.id2label = label_mapping['id2label']
    
    def replace_entities_in_text(self, text):
        """Замена сущностей на метки в тексте"""
        if not text or len(text.strip()) == 0:
            return text
        
        # Токенизация текста на слова
        words = re.findall(r'\w+|[^\w\s]', text)
        
        if not words:
            return text
        
        # Токенизация для модели
        encoding = self.tokenizer(
            words,
            is_split_into_words=True,
            padding=True,
            truncation=True,
            max_length=256,
            return_tensors="pt"
        )
        
        # Предсказание
        with torch.no_grad():
            encoding = {k: v.to(self.device) for k, v in encoding.items()}
            outputs = self.model(**encoding)
            predictions = torch.argmax(outputs.logits, dim=2)
        
        # Преобразование в читаемый формат
        tokens = self.tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])
        predicted_labels = [self.id2label[str(pred.item())] for pred in predictions[0]]
        
        # Объединение результатов и замена сущностей
        result_tokens = []
        current_entity = []
        current_entity_type = None
        
        for token, label in zip(tokens, predicted_labels):
            if token in [self.tokenizer.cls_token, self.tokenizer.sep_token, self.tokenizer.pad_token]:
                continue
                
            if label != 'O':
                # Начало новой сущности
                if not current_entity or label != current_entity_type:
                    if current_entity:
                        # Добавляем предыдущую сущность
                        result_tokens.append(f"[{current_entity_type}]")
                    current_entity = [token.replace('##', '')]
                    current_entity_type = label
                else:
                    # Продолжение сущности
                    current_entity.append(token.replace('##', ''))
            else:
                # Не сущность
                if current_entity:
                    result_tokens.append(f"[{current_entity_type}]")
                    current_entity = []
                    current_entity_type = None
                result_tokens.append(token.replace('##', ''))
        
        # Добавляем последнюю сущность если есть
        if current_entity:
            result_tokens.append(f"[{current_entity_type}]")
        
        # Собираем текст обратно
        result_text = ''
        for token in result_tokens:
            if token in ['.', ',', '!', '?', ';', ':', ')', ']', '}']:
                result_text = result_text.rstrip() + token + ' '
            elif token in ['(', '[', '{']:
                result_text += token
            else:
                result_text += token + ' '
        
        return result_text.strip()

def process_json_file(input_file, model_path, output_file):
    """Обработка JSON файла и замена сущностей"""
    print(f"Загрузка данных из {input_file}...")
    
    # Загрузка исходного JSON
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"Загружено диалогов: {len(data)}")
    
    # Инициализация заменщика сущностей
    print("Загрузка NER модели...")
    replacer = EntityReplacer(model_path)
    
    # Обработка каждого диалога
    processed_data = []
    
    for dialog in tqdm(data, desc="Обработка диалогов"):
        processed_dialog = dialog.copy()
        processed_dialogue = []
        
        # Обработка каждого сообщения в диалоге
        for message in dialog['dialogue']:
            processed_message = message.copy()
            original_text = message['text']
            
            # Замена сущностей в тексте
            processed_text = replacer.replace_entities_in_text(original_text)
            processed_message['text'] = processed_text
            
            processed_dialogue.append(processed_message)
        
        processed_dialog['dialogue'] = processed_dialogue
        processed_data.append(processed_dialog)
    
    # Сохранение результата
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(processed_data, f, ensure_ascii=False, indent=2)
    
    print(f"Обработанные данные сохранены в {output_file}")
    print(f"Обработано диалогов: {len(processed_data)}")
    
    return processed_data

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Замена сущностей на метки в JSON файле')
    parser.add_argument('--input_file', type=str, required=True, help='Входной JSON файл')
    parser.add_argument('--model_path', type=str, required=True, help='Путь к обученной NER модели')
    parser.add_argument('--output_file', type=str, default='filtered_data.json', help='Выходной JSON файл')
    
    args = parser.parse_args()
    
    process_json_file(args.input_file, args.model_path, args.output_file)
