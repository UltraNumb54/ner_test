# improved_first_message_clustering.py
import json
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from tqdm import tqdm
import os
import pymorphy3
import hdbscan
from umap import UMAP

class RussianTextPreprocessor:
    """Класс для предобработки русского текста с лемматизацией"""
    
    def __init__(self):
        self.morph = pymorphy3.MorphAnalyzer()
        # Базовый список стоп-слов (можно расширить при необходимости)
        self.stop_words = {
            'это', 'как', 'так', 'и', 'в', 'над', 'к', 'до', 'не', 'на', 'но', 'за', 'то', 'с', 'ли', 'а', 'во', 'от',
            'со', 'для', 'о', 'же', 'ну', 'вы', 'бы', 'что', 'кто', 'он', 'она', 'они', 'мы', 'вас', 'их', 'его', 'её',
            'мне', 'тебе', 'ему', 'ей', 'нам', 'вам', 'им', 'меня', 'тебя', 'нас', 'них', 'мой', 'твой', 'наш', 'ваш',
            'свой', 'сам', 'сама', 'сами', 'себя', 'себе', 'какой', 'который', 'где', 'куда', 'когда', 'зачем', 'почему',
            'как', 'сколько', 'тот', 'этот', 'такой', 'вот', 'тут', 'здесь', 'там', 'тогда', 'поэтому', 'итак', 'однако',
            'может', 'конечно', 'вероятно', 'точно', 'просто', 'особенно', 'очень', 'слишком', 'совсем', 'чуть', 'почти',
            'именно', 'только', 'лишь', 'еще', 'уже', 'тоже', 'также', 'все', 'всё', 'весь', 'вся', 'каждый', 'любой',
            'самый', 'другой', 'иной', 'последний', 'первый', 'год', 'лет', 'время', 'раз', 'человек', 'дело', 'жизнь',
            'день', 'рука', 'работа', 'слово', 'место', 'лицо', 'друг', 'глаз', 'вопрос', 'дом', 'сторона', 'страна',
            'мир', 'случай', 'голова', 'ребенок', 'сила', 'конец', 'вид', 'система', 'часть', 'город', 'отношение',
            'женщина', 'деньги', 'земля', 'машина', 'вода', 'отец', 'проблема', 'час', 'право', 'нога', 'решение',
            'дверь', 'образ', 'история', 'власть', 'закон', 'война', 'голос', 'тысяча', 'книга', 'возможность',
            'результат', 'ночь', 'стол', 'имя', 'область', 'статья', 'число', 'компания', 'народ', 'жена', 'группа',
            'развитие', 'процесс', 'суд', 'условие', 'средство', 'начало', 'свет', 'пора', 'путь', 'душа', 'уровень',
            'форма', 'связь', 'минута', 'улица', 'вечер', 'качество', 'мысль', 'дорога', 'мать', 'действие', 'месяц',
            'государство', 'язык', 'любовь', 'взгляд', 'мама', 'век', 'школа', 'цель', 'общество', 'деятельность',
            'пожалуйста', 'здравствуйте', 'спасибо', 'помощь', 'информация', 'помогите', 'подскажите', 'скажите'
        }
    
    def lemmatize_text(self, text):
        """Лемматизация русского текста"""
        if not text or not isinstance(text, str):
            return ""
        
        # Очистка текста
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text.split()
        
        # Лемматизация и фильтрация стоп-слов
        lemmas = []
        for word in words:
            if word and word not in self.stop_words and len(word) > 2:
                parsed = self.morph.parse(word)[0]
                lemma = parsed.normal_form
                lemmas.append(lemma)
        
        return " ".join(lemmas)
    
    def preprocess_batch(self, texts):
        """Предобработка батча текстов"""
        return [self.lemmatize_text(text) for text in texts]

class ImprovedFirstMessageClustering:
    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2'):
        """Инициализация с улучшенной моделью"""
        print(f"Загрузка улучшенной модели {model_name}")
        self.model = SentenceTransformer(model_name)
        self.preprocessor = RussianTextPreprocessor()
        print("Модель и препроцессор загружены успешно")
    
    def load_data(self, clustering_file, analysis_file):
        """Загрузка данных для кластеризации по первым сообщениям"""
        print("Загрузка данных...")
        
        with open(clustering_file, 'r', encoding='utf-8') as f:
            clustering_data = json.load(f)
        
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        
        # Создаем словарь для быстрого доступа к полным диалогам
        analysis_dict = {item['dialog_id']: item for item in analysis_data}
        
        texts = []
        original_texts = []
        dialog_info = []
        
        for item in clustering_data:
            dialog_id = item.get('dialog_id')
            first_message = item.get('first_message_replaced', '')
            
            if first_message and len(first_message.strip()) > 10:
                texts.append(first_message.strip())
                original_texts.append(first_message.strip())
                
                full_dialog = analysis_dict.get(dialog_id, {})
                
                dialog_info.append({
                    'dialog_id': dialog_id,
                    'first_message': first_message,
                    'full_text': item.get('full_text_replaced', ''),
                    'full_dialog': full_dialog,
                    'original_topic': full_dialog.get('topic', 'unknown'),
                    'source': full_dialog.get('source', 'unknown')
                })
        
        print(f"Загружено {len(texts)} первых сообщений")
        
        # Предобработка текстов
        print("Предобработка текстов (лемматизация)...")
        processed_texts = self.preprocessor.preprocess_batch(texts)
        
        return processed_texts, original_texts, dialog_info
    
    def create_embeddings(self, texts, batch_size=32):
        """Создание векторных представлений текстов"""
        print("Создание эмбеддингов...")
        
        embeddings = self.model.encode(
            texts, 
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # Нормализация для лучшей кластеризации
        )
        
        print(f"Создано эмбеддингов: {embeddings.shape}")
        return embeddings
    
    def find_optimal_k(self, embeddings, max_k=15):
        """Поиск оптимального количества кластеров для алгоритмов, требующих k"""
        if len(embeddings) < 3:
            return 2
        
        silhouette_scores = []
        k_range = range(2, min(max_k + 1, len(embeddings)))
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(embeddings)
            
            if len(set(labels)) > 1:
                score = silhouette_score(embeddings, labels)
                silhouette_scores.append(score)
            else:
                silhouette_scores.append(0)
        
        optimal_k = k_range[np.argmax(silhouette_scores)]
        print(f"Оптимальное k (силуэтный анализ): {optimal_k}")
        return optimal_k
    
    def find_epsilon_for_dbscan(self, embeddings, k=5):
        """Поиск оптимального epsilon для DBSCAN"""
        neighbors = NearestNeighbors(n_neighbors=k)
        neighbors_fit = neighbors.fit(embeddings)
        distances, indices = neighbors_fit.kneighbors(embeddings)
        distances = np.sort(distances[:, k-1], axis=0)
        
        # Ищем "локоть" на графике расстояний
        gradients = np.diff(distances)
        optimal_idx = np.argmax(gradients) + 1
        epsilon = distances[optimal_idx]
        
        print(f"Оптимальный epsilon для DBSCAN: {epsilon:.4f}")
        return epsilon
    
    def compare_clustering_algorithms(self, embeddings):
        """Сравнение разных алгоритмов кластеризации"""
        print("\nСравнение алгоритмов кластеризации...")
        
        results = {}
        
        # 1. HDBSCAN (рекомендуется для автоматического определения кластеров)
        print("Запуск HDBSCAN...")
        hdbscan_clusterer = hdbscan.HDBSCAN(
            min_cluster_size=5,
            min_samples=3,
            cluster_selection_epsilon=0.1,
            metric='euclidean'
        )
        hdbscan_labels = hdbscan_clusterer.fit_predict(embeddings)
        n_clusters_hdbscan = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)
        
        if n_clusters_hdbscan > 1:
            hdbscan_silhouette = silhouette_score(embeddings, hdbscan_labels)
        else:
            hdbscan_silhouette = 0
        
        results['HDBSCAN'] = {
            'labels': hdbscan_labels,
            'n_clusters': n_clusters_hdbscan,
            'silhouette': hdbscan_silhouette,
            'n_noise': np.sum(hdbscan_labels == -1)
        }
        
        # 2. DBSCAN
        print("Запуск DBSCAN...")
        epsilon = self.find_epsilon_for_dbscan(embeddings)
        dbscan = DBSCAN(eps=epsilon, min_samples=3, metric='euclidean')
        dbscan_labels = dbscan.fit_predict(embeddings)
        n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
        
        if n_clusters_dbscan > 1:
            dbscan_silhouette = silhouette_score(embeddings, dbscan_labels)
        else:
            dbscan_silhouette = 0
        
        results['DBSCAN'] = {
            'labels': dbscan_labels,
            'n_clusters': n_clusters_dbscan,
            'silhouette': dbscan_silhouette,
            'n_noise': np.sum(dbscan_labels == -1)
        }
        
        # 3. Agglomerative Clustering
        print("Запуск Agglomerative Clustering...")
        optimal_k = self.find_optimal_k(embeddings)
        agglo = AgglomerativeClustering(n_clusters=optimal_k, metric='euclidean', linkage='ward')
        agglo_labels = agglo.fit_predict(embeddings)
        
        if optimal_k > 1:
            agglo_silhouette = silhouette_score(embeddings, agglo_labels)
        else:
            agglo_silhouette = 0
        
        results['Agglomerative'] = {
            'labels': agglo_labels,
            'n_clusters': optimal_k,
            'silhouette': agglo_silhouette,
            'n_noise': 0
        }
        
        # 4. Gaussian Mixture Models
        print("Запуск Gaussian Mixture Models...")
        gmm = GaussianMixture(n_components=optimal_k, random_state=42, n_init=3)
        gmm_labels = gmm.fit_predict(embeddings)
        
        if optimal_k > 1:
            gmm_silhouette = silhouette_score(embeddings, gmm_labels)
        else:
            gmm_silhouette = 0
        
        results['GMM'] = {
            'labels': gmm_labels,
            'n_clusters': optimal_k,
            'silhouette': gmm_silhouette,
            'n_noise': 0
        }
        
        # Вывод сравнения
        print("\n" + "="*60)
        print("СРАВНЕНИЕ АЛГОРИТМОВ КЛАСТЕРИЗАЦИИ")
        print("="*60)
        for algo, metrics in results.items():
            print(f"{algo:15} | Кластеров: {metrics['n_clusters']:2} | "
                  f"Silhouette: {metrics['silhouette']:.4f} | "
                  f"Шум: {metrics['n_noise']}")
        
        # Выбор лучшего алгоритма
        best_algo = max(results.items(), key=lambda x: x[1]['silhouette'] if x[1]['n_clusters'] > 1 else -1)
        print(f"\nЛучший алгоритм: {best_algo[0]} (Silhouette: {best_algo[1]['silhouette']:.4f})")
        
        return results, best_algo[0]
    
    def extract_keywords(self, texts, n_keywords=10):
        """Извлечение ключевых слов из текстов"""
        all_text = " ".join(texts).lower()
        
        # Токенизация с учетом лемматизированного текста
        words = re.findall(r'\b[а-яё]{3,}\b', all_text)
        
        # Фильтрация стоп-слов
        word_counts = Counter(words)
        filtered_words = {word: count for word, count in word_counts.items() 
                         if word not in self.preprocessor.stop_words and count > 1}
        
        return Counter(filtered_words).most_common(n_keywords)
    
    def analyze_clusters(self, texts, original_texts, labels, dialog_info):
        """Анализ кластеров"""
        print("Анализ кластеров...")
        
        cluster_analysis = {}
        
        for cluster_id in set(labels):
            if cluster_id == -1:  # Пропускаем шум
                continue
                
            cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]
            cluster_texts = [texts[i] for i in cluster_indices]
            cluster_original = [original_texts[i] for i in cluster_indices]
            cluster_dialogs = [dialog_info[i] for i in cluster_indices]
            
            # Ключевые слова
            keywords = self.extract_keywords(cluster_texts)
            
            # Анализ меток
            metka_stats = self._analyze_metkas(cluster_original)
            
            # Примеры сообщений
            sample_messages = cluster_original[:3]
            
            cluster_analysis[cluster_id] = {
                'size': len(cluster_texts),
                'keywords': keywords,
                'sample_messages': sample_messages,
                'metka_stats': metka_stats,
                'avg_message_length': np.mean([len(text) for text in cluster_original])
            }
        
        return cluster_analysis
    
    def _analyze_metkas(self, texts):
        """Анализ частотности меток"""
        metkas = ['[ФИО]', '[Телефон]', '[Email]', '[Дата]', '[Сумма]', 
                 '[Организация]', '[Адрес]', '[Показания]', '[Номер лицевого счёта]']
        metka_counts = {metka: 0 for metka in metkas}
        
        for text in texts:
            for metka in metkas:
                if metka in text:
                    metka_counts[metka] += 1
        
        return {k: v for k, v in metka_counts.items() if v > 0}
    
    def visualize_results(self, embeddings, results, best_algo):
        """Визуализация результатов всех алгоритмов"""
        print("Создание визуализаций...")
        
        # UMAP для лучшей визуализации
        umap_reducer = UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)
        embeddings_2d = umap_reducer.fit_transform(embeddings)
        
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        axes = axes.flatten()
        
        algorithms = list(results.keys())
        
        for idx, algo in enumerate(algorithms):
            ax = axes[idx]
            labels = results[algo]['labels']
            
            scatter = ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                               c=labels, cmap='tab10', alpha=0.7, s=50)
            ax.set_title(f'{algo}\n'
                        f'Кластеров: {results[algo]["n_clusters"]}, '
                        f'Silhouette: {results[algo]["silhouette"]:.3f}\n'
                        f'Шум: {results[algo]["n_noise"]}',
                        fontsize=12, fontweight='bold' if algo == best_algo else 'normal')
            ax.grid(True, alpha=0.3)
            
            # Добавляем цветовую легенду только если есть кластеры
            unique_labels = set(labels)
            if len(unique_labels) > 1:
                plt.colorbar(scatter, ax=ax)
        
        plt.tight_layout()
        plt.savefig('clustering_algorithms_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("Сравнительная визуализация сохранена: clustering_algorithms_comparison.png")
    
    def save_results(self, dialog_info, labels, cluster_analysis, output_file):
        """Сохранение результатов кластеризации"""
        print("Сохранение результатов...")
        
        results = []
        for i, (item, label) in enumerate(zip(dialog_info, labels)):
            cluster_info = cluster_analysis.get(label, {'size': 0, 'keywords': []})
            
            results.append({
                'dialog_id': item['dialog_id'],
                'cluster_id': int(label),
                'cluster_size': cluster_info['size'],
                'first_message_replaced': item['first_message'],
                'full_dialog': item['full_dialog'],
                'keywords': [word for word, count in cluster_info['keywords'][:5]],
                'original_topic': item.get('original_topic', 'unknown'),
                'source': item.get('source', 'unknown'),
                'is_noise': (label == -1)
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self._print_statistics(results, cluster_analysis)
        
        return results
    
    def _print_statistics(self, results, cluster_analysis):
        """Вывод статистики кластеризации"""
        print("\n" + "="*60)
        print("СТАТИСТИКА КЛАСТЕРИЗАЦИИ ПО ПЕРВЫМ СООБЩЕНИЯМ")
        print("="*60)
        
        total_dialogs = len(results)
        noise_count = sum(1 for r in results if r['is_noise'])
        clustered_count = total_dialogs - noise_count
        
        print(f"Всего диалогов: {total_dialogs}")
        print(f"Успешно кластеризовано: {clustered_count}")
        print(f"Не кластеризовано (шум): {noise_count}")
        print(f"Количество кластеров: {len(cluster_analysis)}")
        
        print("\nРаспределение по кластерам:")
        for cluster_id, analysis in sorted(cluster_analysis.items()):
            print(f"  Кластер {cluster_id}: {analysis['size']} диалогов")
            print(f"    Ключевые слова: {[word for word, count in analysis['keywords'][:3]]}")
            if analysis['metka_stats']:
                print(f"    Частые метки: {analysis['metka_stats']}")
            print(f"    Примеры сообщений:")
            for i, msg in enumerate(analysis['sample_messages'][:2]):
                preview = msg[:100] + "..." if len(msg) > 100 else msg
                print(f"      {i+1}. {preview}")

def main():
    """Основная функция улучшенной кластеризации"""
    
    # Конфигурация
    CLUSTERING_FILE = "clustering_data.json"
    ANALYSIS_FILE = "analysis_data_replaced.json"
    OUTPUT_FILE = "improved_clustering_results_first_messages.json"
    
    print("=" * 60)
    print("УЛУЧШЕННАЯ КЛАСТЕРИЗАЦИЯ ПО ПЕРВЫМ СООБЩЕНИЯМ")
    print("=" * 60)
    print("Улучшения:")
    print("  - Лемматизация русского текста")
    print("  - Модель: paraphrase-multilingual-mpnet-base-v2")
    print("  - Сравнение 4 алгоритмов кластеризации")
    print("  - Автоматический подбор параметров")
    print("=" * 60)
    
    # Инициализация улучшенного кластеризатора
    clusterer = ImprovedFirstMessageClustering()
    
    # Загрузка и предобработка данных
    processed_texts, original_texts, dialog_info = clusterer.load_data(CLUSTERING_FILE, ANALYSIS_FILE)
    
    if len(processed_texts) < 5:
        print("Недостаточно данных для кластеризации (нужно минимум 5 текстов)")
        return
    
    # Создание эмбеддингов
    embeddings = clusterer.create_embeddings(processed_texts)
    
    # Сравнение алгоритмов кластеризации
    results, best_algo = clusterer.compare_clustering_algorithms(embeddings)
    
    # Визуализация результатов
    clusterer.visualize_results(embeddings, results, best_algo)
    
    # Используем лучший алгоритм для финальных результатов
    best_labels = results[best_algo]['labels']
    
    # Анализ кластеров
    cluster_analysis = clusterer.analyze_clusters(processed_texts, original_texts, best_labels, dialog_info)
    
    # Сохранение результатов
    final_results = clusterer.save_results(dialog_info, best_labels, cluster_analysis, OUTPUT_FILE)
    
    print(f"\nКластеризация завершена! Результаты сохранены в {OUTPUT_FILE}")
    print(f"Использован алгоритм: {best_algo}")

if __name__ == "__main__":
    main()
