# clustering_first_messages_enhanced.py
import json
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from tqdm import tqdm
import os

class FirstMessageClusteringEnhanced:
    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):
        """Инициализация модели для создания эмбеддингов"""
        print(f"Загрузка модели {model_name}")
        self.model = SentenceTransformer(model_name)
        print("Модель загружена успешно")
    
    def load_data(self, clustering_file, analysis_file):
        """Загрузка данных для кластеризации по первым сообщениям"""
        print("Загрузка данных...")
        
        # Загрузка данных для кластеризации
        with open(clustering_file, 'r', encoding='utf-8') as f:
            clustering_data = json.load(f)
        
        # Загрузка полных диалогов для анализа
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        
        # Создаем словарь для быстрого доступа к полным диалогам
        analysis_dict = {item['dialog_id']: item for item in analysis_data}
        
        texts = []
        dialog_info = []
        
        for item in clustering_data:
            dialog_id = item.get('dialog_id')
            first_message = item.get('first_message_replaced', '')
            full_text = item.get('full_text_replaced', '')
            
            if first_message and len(first_message.strip()) > 10:  # Минимальная длина
                texts.append(first_message.strip())
                
                # Получаем полный диалог с ролями
                full_dialog = analysis_dict.get(dialog_id, {})
                
                dialog_info.append({
                    'dialog_id': dialog_id,
                    'first_message': first_message,
                    'full_text': full_text,
                    'full_dialog': full_dialog,  # Полный диалог с ролями
                    'original_topic': full_dialog.get('topic', 'unknown'),
                    'source': full_dialog.get('source', 'unknown')
                })
        
        print(f"Загружено {len(texts)} первых сообщений для кластеризации")
        return texts, dialog_info
    
    def create_embeddings(self, texts, batch_size=32):
        """Создание векторных представлений текстов"""
        print("Создание эмбеддингов...")
        
        embeddings = self.model.encode(
            texts, 
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        print(f"Создано эмбеддингов: {embeddings.shape}")
        return embeddings
    
    def find_optimal_clusters(self, embeddings, max_k=15):
        """Поиск оптимального количества кластеров методом локтя и силуэтного анализа"""
        print("Поиск оптимального количества кластеров...")
        
        if len(embeddings) < 3:
            print("Недостаточно данных для анализа кластеров")
            return 2
        
        wcss = []  # Within-Cluster Sum of Square
        silhouette_scores = []
        k_range = range(2, min(max_k + 1, len(embeddings)))
        
        for k in tqdm(k_range, desc="Анализ кластеров"):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(embeddings)
            wcss.append(kmeans.inertia_)
            
            if len(set(labels)) > 1:
                score = silhouette_score(embeddings, labels)
                silhouette_scores.append(score)
            else:
                silhouette_scores.append(0)
        
        # Метод локтя: находим "локоть" на графике WCSS
        differences = []
        for i in range(1, len(wcss)):
            differences.append(wcss[i-1] - wcss[i])
        
        # Находим точку, где разница начинает выравниваться
        elbow_k = 2
        if differences:
            avg_reduction = np.mean(differences)
            for i, diff in enumerate(differences):
                if diff < avg_reduction * 0.5:  # Когда снижение становится меньше половины среднего
                    elbow_k = i + 2
                    break
        
        # Силуэтный анализ
        if silhouette_scores:
            silhouette_k = k_range[np.argmax(silhouette_scores)]
        else:
            silhouette_k = 2
        
        # Выбираем оптимальное k
        optimal_k = max(elbow_k, silhouette_k)
        
        print(f"Оптимальное количество кластеров:")
        print(f"  Метод локтя: {elbow_k}")
        print(f"  Силуэтный анализ: {silhouette_k}")
        print(f"  Выбрано: {optimal_k}")
        
        # Визуализация выбора k
        self._plot_optimal_clusters(k_range, wcss, silhouette_scores, elbow_k, silhouette_k)
        
        return optimal_k
    
    def _plot_optimal_clusters(self, k_range, wcss, silhouette_scores, elbow_k, silhouette_k):
        """Визуализация выбора оптимального количества кластеров"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # График метода локтя
        ax1.plot(k_range, wcss, 'bo-')
        ax1.axvline(x=elbow_k, color='red', linestyle='--', alpha=0.7, label=f'Локоть: k={elbow_k}')
        ax1.set_xlabel('Количество кластеров')
        ax1.set_ylabel('WCSS (Within-Cluster Sum of Square)')
        ax1.set_title('Метод локтя')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # График силуэтного анализа
        ax2.plot(k_range, silhouette_scores, 'go-')
        ax2.axvline(x=silhouette_k, color='red', linestyle='--', alpha=0.7, label=f'Оптимум: k={silhouette_k}')
        ax2.set_xlabel('Количество кластеров')
        ax2.set_ylabel('Silhouette Score')
        ax2.set_title('Силуэтный анализ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('clustering_optimal_k_first_messages.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("График выбора оптимального k сохранен: clustering_optimal_k_first_messages.png")
    
    def perform_clustering(self, embeddings, n_clusters=None):
        """Выполнение кластеризации K-means с расчетом уверенности"""
        if n_clusters is None:
            n_clusters = min(10, max(2, len(embeddings) // 10))
        
        print(f"Выполнение K-means кластеризации с {n_clusters} кластерами...")
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings)
        
        # Расчет уверенности принадлежности к кластеру
        distances = kmeans.transform(embeddings)
        min_distances = np.min(distances, axis=1)
        confidence_scores = 1 / (1 + min_distances)  # Преобразуем расстояние в уверенность
        
        # Нормализуем уверенность от 0 до 1
        confidence_scores = (confidence_scores - confidence_scores.min()) / (confidence_scores.max() - confidence_scores.min())
        
        # Оценка качества
        if len(set(labels)) > 1:
            score = silhouette_score(embeddings, labels)
            print(f"Silhouette Score: {score:.4f}")
        else:
            print("Создан только один кластер")
        
        return labels, kmeans, confidence_scores
    
    def extract_keywords(self, texts, n_keywords=20):  # Увеличили количество ключевых слов
        """Извлечение ключевых слов из текстов"""
        all_text = " ".join(texts).lower()
        
        # Токенизация и очистка
        words = re.findall(r'\b[а-яё]{3,}\b', all_text)
        
        # Расширенный список стоп-слов
        stop_words = {
            'этот', 'такой', 'какой', 'который', 'очень', 'много', 'можно', 
            'нужно', 'должен', 'хочу', 'хотеть', 'хотят', 'свой', 'мочь',
            'будет', 'есть', 'быть', 'сказать', 'говорить', 'пожалуйста',
            'здравствуйте', 'спасибо', 'проблема', 'вопрос', 'помощь',
            'интересует', 'узнать', 'поэтому', 'когда', 'потом', 'сейчас',
            'только', 'хотя', 'вообще', 'вполне', 'вроде', 'далее', 'ещё',
            'именно', 'какоето', 'какойто', 'какаято', 'какието', 'ктото',
            'чтолибо', 'чтонибудь', 'какойнибудь', 'сразу', 'таки', 'уже',
            'часто', 'иногда', 'обычно', 'точно', 'конечно', 'вообще',
            'возможно', 'наверное', 'probably', 'maybe', 'perhaps'
        }
        
        # Фильтрация стоп-слов и подсчет частот
        word_counts = Counter(words)
        filtered_words = {word: count for word, count in word_counts.items() 
                         if word not in stop_words and count > 1}
        
        return Counter(filtered_words).most_common(n_keywords)
    
    def calculate_cluster_quality_metrics(self, embeddings, labels, confidence_scores):
        """Расчет метрик качества для каждого кластера"""
        print("Расчет метрик качества кластеров...")
        
        cluster_metrics = {}
        
        for cluster_id in set(labels):
            cluster_indices = np.where(labels == cluster_id)[0]
            cluster_embeddings = embeddings[cluster_indices]
            
            # Средняя уверенность в кластере
            avg_confidence = np.mean(confidence_scores[cluster_indices])
            
            # Когезия кластера (среднее расстояние до центроида)
            if len(cluster_embeddings) > 1:
                centroid = np.mean(cluster_embeddings, axis=0)
                distances = np.linalg.norm(cluster_embeddings - centroid, axis=1)
                cohesion = np.mean(distances)
                
                # Относительная когезия (нормализованная)
                max_possible_distance = np.max(np.linalg.norm(embeddings - np.mean(embeddings, axis=0), axis=1))
                relative_cohesion = cohesion / max_possible_distance if max_possible_distance > 0 else 0
            else:
                cohesion = 0
                relative_cohesion = 0
            
            # Силуэтный score для кластера
            if len(set(labels)) > 1:
                cluster_silhouette = np.mean(silhouette_samples(embeddings, labels)[cluster_indices])
            else:
                cluster_silhouette = 0
            
            cluster_metrics[cluster_id] = {
                'size': len(cluster_indices),
                'avg_confidence': avg_confidence,
                'cohesion': cohesion,
                'relative_cohesion': relative_cohesion,
                'silhouette_score': cluster_silhouette,
                'quality_score': avg_confidence * (1 - relative_cohesion)  # Композитная оценка качества
            }
        
        return cluster_metrics
    
    def analyze_clusters(self, texts, labels, dialog_info, confidence_scores, cluster_metrics):
        """Анализ кластеров и извлечение ключевых характеристик"""
        print("Анализ кластеров...")
        
        cluster_analysis = {}
        
        for cluster_id in set(labels):
            cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]
            cluster_texts = [texts[i] for i in cluster_indices]
            cluster_dialogs = [dialog_info[i] for i in cluster_indices]
            cluster_confidences = [confidence_scores[i] for i in cluster_indices]
            
            # Сортируем по уверенности для получения лучших примеров
            sorted_indices = np.argsort(cluster_confidences)[::-1]  # По убыванию уверенности
            best_examples = [cluster_texts[i] for i in sorted_indices[:5]]
            worst_examples = [cluster_texts[i] for i in sorted_indices[-3:]] if len(sorted_indices) > 3 else []
            
            # Ключевые слова (увеличили количество)
            keywords = self.extract_keywords(cluster_texts, n_keywords=20)
            
            # Анализ частотности меток
            metka_stats = self._analyze_metkas(cluster_texts)
            
            # Анализ тематик исходных диалогов
            topic_stats = self._analyze_topics(cluster_dialogs)
            
            cluster_analysis[cluster_id] = {
                'size': len(cluster_texts),
                'keywords': keywords,
                'best_examples': best_examples,  # Наиболее типичные сообщения
                'worst_examples': worst_examples,  # Наименее типичные сообщения
                'metka_stats': metka_stats,
                'topic_stats': topic_stats,
                'avg_message_length': np.mean([len(text) for text in cluster_texts]),
                'metrics': cluster_metrics[cluster_id]
            }
        
        return cluster_analysis
    
    def _analyze_metkas(self, texts):
        """Анализ частотности меток в кластере"""
        metkas = ['[ФИО]', '[Телефон]', '[Email]', '[Дата]', '[Сумма]', 
                 '[Организация]', '[Адрес]', '[Показания]', '[Номер лицевого счёта]',
                 '[Время]', '[Место]', '[Документ]', '[Сайт]', '[Ссылка]']
        metka_counts = {metka: 0 for metka in metkas}
        
        for text in texts:
            for metka in metkas:
                if metka in text:
                    metka_counts[metka] += 1
        
        # Возвращаем только метки, которые встречаются хотя бы в 10% текстов
        threshold = len(texts) * 0.1
        return {k: v for k, v in metka_counts.items() if v > threshold}
    
    def _analyze_topics(self, dialogs):
        """Анализ распределения исходных тематик в кластере"""
        topics = [dialog.get('original_topic', 'unknown') for dialog in dialogs]
        topic_counts = Counter(topics)
        return dict(topic_counts.most_common(10))
    
    def visualize_clusters(self, embeddings, labels, cluster_analysis, confidence_scores):
        """Визуализация кластеров с помощью PCA с учетом уверенности"""
        print("Визуализация кластеров...")
        
        # PCA для визуализации
        pca = PCA(n_components=2, random_state=42)
        embeddings_2d = pca.fit_transform(embeddings)
        
        # Создаем DataFrame для удобства построения графиков
        df = pd.DataFrame({
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1],
            'cluster': labels,
            'confidence': confidence_scores
        })
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
        
        # 1. Точечная диаграмма кластеров с цветом по уверенности
        scatter = ax1.scatter(df['x'], df['y'], c=df['cluster'], cmap='tab10', 
                            alpha=0.7, s=50)
        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        ax1.set_title('Визуализация кластеров (PCA)')
        ax1.legend(*scatter.legend_elements(), title="Кластеры")
        ax1.grid(True, alpha=0.3)
        
        # 2. Точечная диаграмма с прозрачностью по уверенности
        scatter2 = ax2.scatter(df['x'], df['y'], c=df['cluster'], cmap='tab10',
                             alpha=df['confidence'], s=50)
        ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        ax2.set_title('Уверенность принадлежности к кластеру\n(альфа-канал = уверенность)')
        ax2.grid(True, alpha=0.3)
        
        # 3. Размеры кластеров и средняя уверенность
        cluster_sizes = {k: v['size'] for k, v in cluster_analysis.items()}
        cluster_confidences = {k: v['metrics']['avg_confidence'] for k, v in cluster_analysis.items()}
        
        clusters = list(cluster_sizes.keys())
        sizes = list(cluster_sizes.values())
        confidences = [cluster_confidences[k] for k in clusters]
        
        bars = ax3.bar(clusters, sizes, color=plt.cm.tab10(range(len(clusters))), alpha=0.7)
        ax3.set_xlabel('Номер кластера')
        ax3.set_ylabel('Количество диалогов', color='blue')
        ax3.tick_params(axis='y', labelcolor='blue')
        ax3.set_title('Распределение диалогов по кластерам')
        
        # Добавляем линию уверенности
        ax3_conf = ax3.twinx()
        ax3_conf.plot(clusters, confidences, 'ro-', linewidth=2, markersize=8, label='Средняя уверенность')
        ax3_conf.set_ylabel('Средняя уверенность', color='red')
        ax3_conf.tick_params(axis='y', labelcolor='red')
        ax3_conf.set_ylim(0, 1)
        ax3_conf.legend(loc='upper right')
        
        # 4. Качество кластеров
        quality_scores = [cluster_analysis[k]['metrics']['quality_score'] for k in clusters]
        cohesion_scores = [cluster_analysis[k]['metrics']['relative_cohesion'] for k in clusters]
        
        x_pos = np.arange(len(clusters))
        width = 0.35
        
        ax4.bar(x_pos - width/2, quality_scores, width, label='Оценка качества', alpha=0.7)
        ax4.bar(x_pos + width/2, cohesion_scores, width, label='Относительная когезия', alpha=0.7)
        ax4.set_xlabel('Номер кластера')
        ax4.set_ylabel('Оценка')
        ax4.set_title('Качество кластеров')
        ax4.set_xticks(x_pos)
        ax4.set_xticklabels(clusters)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('clustering_visualization_first_messages_enhanced.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("Графики кластеризации сохранены: clustering_visualization_first_messages_enhanced.png")
        
        # Выводим информацию о PCA
        print(f"PCA объясненная дисперсия: PC1={pca.explained_variance_ratio_[0]:.2%}, PC2={pca.explained_variance_ratio_[1]:.2%}")
    
    def save_results(self, dialog_info, labels, confidence_scores, cluster_analysis, output_file):
        """Сохранение результатов кластеризации с дополнительной информацией"""
        print("Сохранение результатов...")
        
        results = []
        for i, (item, label, confidence) in enumerate(zip(dialog_info, labels, confidence_scores)):
            cluster_data = cluster_analysis[label]
            
            results.append({
                'dialog_id': item['dialog_id'],
                'cluster_id': int(label),
                'cluster_size': cluster_data['size'],
                'confidence_score': float(confidence),
                'confidence_level': self._get_confidence_level(confidence),
                'first_message_replaced': item['first_message'],
                'full_dialog': item['full_dialog'],
                'keywords': [word for word, count in cluster_data['keywords'][:8]],  # Больше ключевых слов
                'cluster_quality': {
                    'quality_score': float(cluster_data['metrics']['quality_score']),
                    'avg_confidence': float(cluster_data['metrics']['avg_confidence']),
                    'cohesion': float(cluster_data['metrics']['cohesion']),
                    'silhouette_score': float(cluster_data['metrics']['silhouette_score'])
                },
                'original_topic': item.get('original_topic', 'unknown'),
                'source': item.get('source', 'unknown'),
                'is_typical_example': confidence > 0.7,  # Флаг типичного примера
                'cluster_representativeness': self._calculate_representativeness(confidence, cluster_data)
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # Сохраняем сводный отчет по кластерам
        self._save_cluster_summary(cluster_analysis, 'cluster_summary_first_messages.json')
        
        self._print_statistics(results, cluster_analysis)
        
        return results
    
    def _get_confidence_level(self, confidence):
        """Определение уровня уверенности"""
        if confidence > 0.8:
            return "high"
        elif confidence > 0.6:
            return "medium"
        elif confidence > 0.4:
            return "low"
        else:
            return "very_low"
    
    def _calculate_representativeness(self, confidence, cluster_data):
        """Расчет репрезентативности диалога для кластера"""
        base_score = confidence
        # Учитываем размер кластера (в маленьких кластерах выше репрезентативность)
        size_factor = 1.0 if cluster_data['size'] > 10 else 1.2
        return min(1.0, base_score * size_factor)
    
    def _save_cluster_summary(self, cluster_analysis, filename):
        """Сохранение сводного отчета по кластерам"""
        summary = {}
        
        for cluster_id, analysis in cluster_analysis.items():
            summary[cluster_id] = {
                'size': analysis['size'],
                'quality_metrics': analysis['metrics'],
                'top_keywords': [word for word, count in analysis['keywords'][:15]],  # 15 ключевых слов
                'common_metkas': analysis['metka_stats'],
                'topic_distribution': analysis['topic_stats'],
                'best_examples': analysis['best_examples'],
                'worst_examples': analysis['worst_examples'],
                'cluster_characteristics': self._describe_cluster_characteristics(analysis)
            }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"Сводный отчет по кластерам сохранен: {filename}")
    
    def _describe_cluster_characteristics(self, analysis):
        """Описание характеристик кластера"""
        characteristics = []
        
        # На основе ключевых слов
        top_keywords = [word for word, count in analysis['keywords'][:5]]
        if top_keywords:
            characteristics.append(f"Ключевые темы: {', '.join(top_keywords)}")
        
        # На основе меток
        if analysis['metka_stats']:
            common_metkas = list(analysis['metka_stats'].keys())[:3]
            characteristics.append(f"Частые метки: {', '.join(common_metkas)}")
        
        # На основе качества
        metrics = analysis['metrics']
        if metrics['quality_score'] > 0.7:
            characteristics.append("Высококачественный кластер")
        elif metrics['quality_score'] > 0.5:
            characteristics.append("Среднекачественный кластер")
        else:
            characteristics.append("Низкокачественный кластер (размытые границы)")
        
        # На основе размера
        if analysis['size'] > 50:
            characteristics.append("Крупный кластер")
        elif analysis['size'] < 10:
            characteristics.append("Маленький кластер")
        
        return characteristics
    
    def _print_statistics(self, results, cluster_analysis):
        """Вывод статистики кластеризации"""
        print("\n" + "="*80)
        print("РАСШИРЕННАЯ СТАТИСТИКА КЛАСТЕРИЗАЦИИ ПО ПЕРВЫМ СООБЩЕНИЯМ")
        print("="*80)
        print(f"Всего диалогов: {len(results)}")
        print(f"Количество кластеров: {len(cluster_analysis)}")
        
        # Распределение по уровням уверенности
        confidence_levels = [r['confidence_level'] for r in results]
        confidence_dist = Counter(confidence_levels)
        
        print("\nРаспределение по уровням уверенности:")
        for level, count in confidence_dist.most_common():
            print(f"  {level}: {count} диалогов ({count/len(results)*100:.1f}%)")
        
        print("\nДетальный анализ кластеров:")
        for cluster_id, analysis in sorted(cluster_analysis.items()):
            metrics = analysis['metrics']
            print(f"\n  Кластер {cluster_id}:")
            print(f"    Размер: {analysis['size']} диалогов")
            print(f"    Качество: {metrics['quality_score']:.3f} (уверенность: {metrics['avg_confidence']:.3f}, когезия: {metrics['relative_cohesion']:.3f})")
            print(f"    Топ-8 ключевых слов: {[word for word, count in analysis['keywords'][:8]]}")
            
            if analysis['metka_stats']:
                print(f"    Частые метки: {analysis['metka_stats']}")
            
            if analysis['topic_stats']:
                top_topics = list(analysis['topic_stats'].items())[:3]
                print(f"    Основные темы: {top_topics}")
            
            print(f"    Лучшие примеры:")
            for i, msg in enumerate(analysis['best_examples'][:2]):
                preview = msg[:80] + "..." if len(msg) > 80 else msg
                print(f"      {i+1}. {preview}")

def main():
    """Основная функция расширенной кластеризации по первым сообщениям"""
    
    # Конфигурация
    CLUSTERING_FILE = "clustering_data.json"
    ANALYSIS_FILE = "analysis_data_replaced.json"
    OUTPUT_FILE = "clustering_results_first_messages_enhanced.json"
    NUM_CLUSTERS = None  # Автоопределение
    
    print("=" * 80)
    print("РАСШИРЕННАЯ КЛАСТЕРИЗАЦИЯ ПО ПЕРВЫМ СООБЩЕНИЯМ")
    print("=" * 80)
    
    # Инициализация улучшенного кластеризатора
    clusterer = FirstMessageClusteringEnhanced()
    
    # Загрузка данных
    texts, dialog_info = clusterer.load_data(CLUSTERING_FILE, ANALYSIS_FILE)
    
    if len(texts) < 3:
        print("Недостаточно данных для кластеризации (нужно минимум 3 текста)")
        return
    
    # Создание эмбеддингов
    embeddings = clusterer.create_embeddings(texts)
    
    # Определение количества кластеров
    if NUM_CLUSTERS is None:
        NUM_CLUSTERS = clusterer.find_optimal_clusters(embeddings)
    
    # Кластеризация с расчетом уверенности
    labels, kmeans, confidence_scores = clusterer.perform_clustering(embeddings, NUM_CLUSTERS)
    
    # Расчет метрик качества кластеров
    cluster_metrics = clusterer.calculate_cluster_quality_metrics(embeddings, labels, confidence_scores)
    
    # Анализ кластеров
    cluster_analysis = clusterer.analyze_clusters(texts, labels, dialog_info, confidence_scores, cluster_metrics)
    
    # Визуализация
    clusterer.visualize_clusters(embeddings, labels, cluster_analysis, confidence_scores)
    
    # Сохранение результатов
    results = clusterer.save_results(dialog_info, labels, confidence_scores, cluster_analysis, OUTPUT_FILE)
    
    print(f"\nРасширенная кластеризация завершена!")
    print(f"Основные результаты сохранены в {OUTPUT_FILE}")
    print(f"Сводный отчет по кластерам: cluster_summary_first_messages.json")

if __name__ == "__main__":
    main()
