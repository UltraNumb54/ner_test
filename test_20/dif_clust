# gmm_first_message_clustering.py
import json
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from tqdm import tqdm
import os
import pymorphy3

class NumpyEncoder(json.JSONEncoder):
    """Кастомный JSON encoder для обработки numpy типов и bool"""
    def default(self, obj):
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        if isinstance(obj, (np.integer, np.int32, np.int64)):
            return int(obj)
        if isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)

class RussianTextPreprocessor:
    """Класс для предобработки русского текста с лемматизацией"""
    
    def __init__(self):
        self.morph = pymorphy3.MorphAnalyzer()
        self.stop_words = {
            'это', 'как', 'так', 'и', 'в', 'над', 'к', 'до', 'не', 'на', 'но', 'за', 'то', 'с', 'ли', 'а', 'во', 'от',
            'со', 'для', 'о', 'же', 'ну', 'вы', 'бы', 'что', 'кто', 'он', 'она', 'они', 'мы', 'вас', 'их', 'его', 'её',
            'мне', 'тебе', 'ему', 'ей', 'нам', 'вам', 'им', 'меня', 'тебя', 'нас', 'них', 'мой', 'твой', 'наш', 'ваш',
            'свой', 'сам', 'сама', 'сами', 'себя', 'себе', 'какой', 'который', 'где', 'куда', 'когда', 'зачем', 'почему',
            'как', 'сколько', 'тот', 'этот', 'такой', 'вот', 'тут', 'здесь', 'там', 'тогда', 'поэтому', 'итак', 'однако',
            'может', 'конечно', 'вероятно', 'точно', 'просто', 'особенно', 'очень', 'слишком', 'совсем', 'чуть', 'почти',
            'именно', 'только', 'лишь', 'еще', 'уже', 'тоже', 'также', 'все', 'всё', 'весь', 'вся', 'каждый', 'любой',
            'самый', 'другой', 'иной', 'последний', 'первый', 'пожалуйста', 'здравствуйте', 'спасибо', 'помощь', 
            'информация', 'помогите', 'подскажите', 'скажите'
        }
    
    def lemmatize_text(self, text):
        """Лемматизация русского текста"""
        if not text or not isinstance(text, str):
            return ""
        
        # Очистка текста
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text.split()
        
        # Лемматизация и фильтрация стоп-слов
        lemmas = []
        for word in words:
            if word and word not in self.stop_words and len(word) > 2:
                parsed = self.morph.parse(word)[0]
                lemma = parsed.normal_form
                lemmas.append(lemma)
        
        return " ".join(lemmas)
    
    def preprocess_batch(self, texts):
        """Предобработка батча текстов"""
        return [self.lemmatize_text(text) for text in texts]

class GMMFirstMessageClustering:
    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2'):
        """Инициализация с улучшенной моделью"""
        print(f"Загрузка модели {model_name}")
        self.model = SentenceTransformer(model_name)
        self.preprocessor = RussianTextPreprocessor()
        print("Модель и препроцессор загружены успешно")
    
    def load_data(self, clustering_file, analysis_file):
        """Загрузка данных для кластеризации по первым сообщениям"""
        print("Загрузка данных...")
        
        with open(clustering_file, 'r', encoding='utf-8') as f:
            clustering_data = json.load(f)
        
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        
        # Создаем словарь для быстрого доступа к полным диалогам
        analysis_dict = {item['dialog_id']: item for item in analysis_data}
        
        texts = []
        original_texts = []
        dialog_info = []
        
        for item in clustering_data:
            dialog_id = item.get('dialog_id')
            first_message = item.get('first_message_replaced', '')
            
            if first_message and len(first_message.strip()) > 10:
                texts.append(first_message.strip())
                original_texts.append(first_message.strip())
                
                full_dialog = analysis_dict.get(dialog_id, {})
                
                dialog_info.append({
                    'dialog_id': dialog_id,
                    'first_message': first_message,
                    'full_text': item.get('full_text_replaced', ''),
                    'full_dialog': full_dialog,
                    'original_topic': full_dialog.get('topic', 'unknown'),
                    'source': full_dialog.get('source', 'unknown')
                })
        
        print(f"Загружено {len(texts)} первых сообщений")
        
        # Предобработка текстов
        print("Предобработка текстов (лемматизация)...")
        processed_texts = self.preprocessor.preprocess_batch(texts)
        
        return processed_texts, original_texts, dialog_info
    
    def create_embeddings(self, texts, batch_size=32):
        """Создание векторных представлений текстов"""
        print("Создание эмбеддингов...")
        
        embeddings = self.model.encode(
            texts, 
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        print(f"Создано эмбеддингов: {embeddings.shape}")
        return embeddings
    
    def find_optimal_clusters_gmm(self, embeddings, max_k=15):
        """Поиск оптимального количества кластеров для GMM с использованием BIC и силуэтного анализа"""
        print("Поиск оптимального количества кластеров для GMM...")
        
        if len(embeddings) < 3:
            return 2
        
        bic_scores = []
        silhouette_scores = []
        k_range = range(1, min(max_k + 1, len(embeddings)))
        
        for k in tqdm(k_range, desc="Анализ кластеров GMM"):
            try:
                gmm = GaussianMixture(n_components=k, random_state=42, n_init=3)
                labels = gmm.fit_predict(embeddings)
                bic_scores.append(gmm.bic(embeddings))
                
                if len(set(labels)) > 1:
                    score = silhouette_score(embeddings, labels)
                    silhouette_scores.append(score)
                else:
                    silhouette_scores.append(0)
            except Exception as e:
                print(f"Ошибка при k={k}: {e}")
                bic_scores.append(np.inf)
                silhouette_scores.append(0)
        
        # Находим оптимальное k по BIC (минимум) и силуэту (максимум)
        if len(bic_scores) > 1:
            # Исключаем k=1 для BIC анализа
            bic_k = np.argmin(bic_scores[1:]) + 2 if len(bic_scores) > 1 else 2
        else:
            bic_k = 2
            
        if len(silhouette_scores) > 1:
            silhouette_k = np.argmax(silhouette_scores[1:]) + 2 if len(silhouette_scores) > 1 else 2
        else:
            silhouette_k = 2
        
        # Выбираем оптимальное k (предпочтение силуэтному анализу для кластеризации)
        optimal_k = silhouette_k
        
        print(f"Оптимальное количество кластеров:")
        print(f"  BIC анализ: {bic_k}")
        print(f"  Силуэтный анализ: {silhouette_k}")
        print(f"  Выбрано: {optimal_k}")
        
        # Визуализация выбора k
        self._plot_optimal_clusters_gmm(k_range, bic_scores, silhouette_scores, bic_k, silhouette_k)
        
        return optimal_k
    
    def _plot_optimal_clusters_gmm(self, k_range, bic_scores, silhouette_scores, bic_k, silhouette_k):
        """Визуализация выбора оптимального количества кластеров для GMM"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # График BIC
        ax1.plot(k_range, bic_scores, 'bo-')
        ax1.axvline(x=bic_k, color='red', linestyle='--', alpha=0.7, label=f'Оптимум BIC: k={bic_k}')
        ax1.set_xlabel('Количество кластеров')
        ax1.set_ylabel('BIC Score')
        ax1.set_title('BIC анализ для GMM')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # График силуэтного анализа
        ax2.plot(k_range, silhouette_scores, 'go-')
        ax2.axvline(x=silhouette_k, color='red', linestyle='--', alpha=0.7, label=f'Оптимум силуэта: k={silhouette_k}')
        ax2.set_xlabel('Количество кластеров')
        ax2.set_ylabel('Silhouette Score')
        ax2.set_title('Силуэтный анализ для GMM')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('gmm_optimal_k.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("График выбора оптимального k сохранен: gmm_optimal_k.png")
    
    def perform_gmm_clustering(self, embeddings, n_components=None):
        """Выполнение кластеризации Gaussian Mixture Models"""
        if n_components is None:
            n_components = min(10, max(2, len(embeddings) // 10))
        
        print(f"Выполнение GMM кластеризации с {n_components} кластерами...")
        
        gmm = GaussianMixture(
            n_components=n_components, 
            random_state=42, 
            n_init=5,
            covariance_type='full'
        )
        
        # Получаем метки кластеров и вероятности
        labels = gmm.fit_predict(embeddings)
        probabilities = gmm.predict_proba(embeddings)
        
        # Оценка качества
        if len(set(labels)) > 1:
            score = silhouette_score(embeddings, labels)
            print(f"Silhouette Score: {score:.4f}")
        else:
            print("Создан только один кластер")
        
        return labels, gmm, probabilities
    
    def extract_keywords(self, texts, n_keywords=10):
        """Извлечение ключевых слов из текстов"""
        all_text = " ".join(texts).lower()
        
        # Токенизация с учетом лемматизированного текста
        words = re.findall(r'\b[а-яё]{3,}\b', all_text)
        
        # Фильтрация стоп-слов
        word_counts = Counter(words)
        filtered_words = {word: count for word, count in word_counts.items() 
                         if word not in self.preprocessor.stop_words and count > 1}
        
        return Counter(filtered_words).most_common(n_keywords)
    
    def analyze_clusters(self, texts, original_texts, labels, dialog_info):
        """Анализ кластеров"""
        print("Анализ кластеров...")
        
        cluster_analysis = {}
        
        for cluster_id in set(labels):
            cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]
            cluster_texts = [texts[i] for i in cluster_indices]
            cluster_original = [original_texts[i] for i in cluster_indices]
            cluster_dialogs = [dialog_info[i] for i in cluster_indices]
            
            # Ключевые слова
            keywords = self.extract_keywords(cluster_texts)
            
            # Анализ меток
            metka_stats = self._analyze_metkas(cluster_original)
            
            # Примеры сообщений
            sample_messages = cluster_original[:3]
            
            cluster_analysis[cluster_id] = {
                'size': len(cluster_texts),
                'keywords': keywords,
                'sample_messages': sample_messages,
                'metka_stats': metka_stats,
                'avg_message_length': np.mean([len(text) for text in cluster_original])
            }
        
        return cluster_analysis
    
    def _analyze_metkas(self, texts):
        """Анализ частотности меток"""
        metkas = ['[ФИО]', '[Телефон]', '[Email]', '[Дата]', '[Сумма]', 
                 '[Организация]', '[Адрес]', '[Показания]', '[Номер лицевого счёта]']
        metka_counts = {metka: 0 for metka in metkas}
        
        for text in texts:
            for metka in metkas:
                if metka in text:
                    metka_counts[metka] += 1
        
        return {k: v for k, v in metka_counts.items() if v > 0}
    
    def visualize_clusters(self, embeddings, labels, cluster_analysis):
        """Визуализация кластеров GMM"""
        print("Визуализация кластеров GMM...")
        
        # PCA для визуализации
        pca = PCA(n_components=2, random_state=42)
        embeddings_2d = pca.fit_transform(embeddings)
        
        # Создаем DataFrame для удобства построения графиков
        df = pd.DataFrame({
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1],
            'cluster': labels
        })
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
        
        # 1. Точечная диаграмма кластеров
        scatter = ax1.scatter(df['x'], df['y'], c=df['cluster'], cmap='tab10', alpha=0.7, s=50)
        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        ax1.set_title('GMM кластеризация первых сообщений\n(PCA проекция)')
        ax1.legend(*scatter.legend_elements(), title="Кластеры")
        ax1.grid(True, alpha=0.3)
        
        # 2. Размеры кластеров
        cluster_sizes = {k: v['size'] for k, v in cluster_analysis.items()}
        clusters = list(cluster_sizes.keys())
        sizes = list(cluster_sizes.values())
        
        bars = ax2.bar(clusters, sizes, color=plt.cm.tab10(range(len(clusters))))
        ax2.set_xlabel('Номер кластера')
        ax2.set_ylabel('Количество диалогов')
        ax2.set_title('Распределение диалогов по кластерам (GMM)')
        
        # Добавляем подписи на столбцах
        for bar, size in zip(bars, sizes):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{size}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('gmm_clustering_visualization.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("Графики кластеризации сохранены: gmm_clustering_visualization.png")
        print(f"PCA объясненная дисперсия: PC1={pca.explained_variance_ratio_[0]:.2%}, PC2={pca.explained_variance_ratio_[1]:.2%}")
    
    def save_results(self, dialog_info, labels, cluster_analysis, output_file):
        """Сохранение результатов кластеризации с исправленной JSON сериализацией"""
        print("Сохранение результатов...")
        
        results = []
        for i, (item, label) in enumerate(zip(dialog_info, labels)):
            cluster_info = cluster_analysis.get(label, {'size': 0, 'keywords': []})
            
            result_item = {
                'dialog_id': item['dialog_id'],
                'cluster_id': int(label),
                'cluster_size': cluster_info['size'],
                'first_message_replaced': item['first_message'],
                'full_dialog': item['full_dialog'],
                'keywords': [word for word, count in cluster_info['keywords'][:5]],
                'original_topic': item.get('original_topic', 'unknown'),
                'source': item.get('source', 'unknown')
            }
            
            # Явно преобразуем все numpy типы
            results.append(json.loads(json.dumps(result_item, cls=NumpyEncoder)))
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
        
        self._print_statistics(results, cluster_analysis)
        
        return results
    
    def _print_statistics(self, results, cluster_analysis):
        """Вывод статистики кластеризации"""
        print("\n" + "="*60)
        print("СТАТИСТИКА КЛАСТЕРИЗАЦИИ GMM ПО ПЕРВЫМ СООБЩЕНИЯМ")
        print("="*60)
        
        total_dialogs = len(results)
        
        print(f"Всего диалогов: {total_dialogs}")
        print(f"Количество кластеров: {len(cluster_analysis)}")
        
        print("\nРаспределение по кластерам:")
        for cluster_id, analysis in sorted(cluster_analysis.items()):
            print(f"  Кластер {cluster_id}: {analysis['size']} диалогов")
            print(f"    Ключевые слова: {[word for word, count in analysis['keywords'][:3]]}")
            if analysis['metka_stats']:
                print(f"    Частые метки: {analysis['metka_stats']}")
            print(f"    Примеры сообщений:")
            for i, msg in enumerate(analysis['sample_messages'][:2]):
                preview = msg[:100] + "..." if len(msg) > 100 else msg
                print(f"      {i+1}. {preview}")

def main():
    """Основная функция кластеризации GMM по первым сообщениям"""
    
    # Конфигурация
    CLUSTERING_FILE = "clustering_data.json"
    ANALYSIS_FILE = "analysis_data_replaced.json"
    OUTPUT_FILE = "gmm_clustering_results_first_messages.json"
    
    print("=" * 60)
    print("GMM КЛАСТЕРИЗАЦИЯ ПО ПЕРВЫМ СООБЩЕНИЯМ")
    print("=" * 60)
    print("Особенности:")
    print("  - Лемматизация русского текста")
    print("  - Модель: paraphrase-multilingual-mpnet-base-v2")
    print("  - Gaussian Mixture Models (вероятностный подход)")
    print("  - Автоматический подбор числа кластеров")
    print("  - Исправленная JSON сериализация")
    print("=" * 60)
    
    # Инициализация кластеризатора
    clusterer = GMMFirstMessageClustering()
    
    # Загрузка и предобработка данных
    processed_texts, original_texts, dialog_info = clusterer.load_data(CLUSTERING_FILE, ANALYSIS_FILE)
    
    if len(processed_texts) < 5:
        print("Недостаточно данных для кластеризации (нужно минимум 5 текстов)")
        return
    
    # Создание эмбеддингов
    embeddings = clusterer.create_embeddings(processed_texts)
    
    # Определение оптимального количества кластеров
    optimal_k = clusterer.find_optimal_clusters_gmm(embeddings)
    
    # Кластеризация GMM
    labels, gmm, probabilities = clusterer.perform_gmm_clustering(embeddings, optimal_k)
    
    # Анализ кластеров
    cluster_analysis = clusterer.analyze_clusters(processed_texts, original_texts, labels, dialog_info)
    
    # Визуализация
    clusterer.visualize_clusters(embeddings, labels, cluster_analysis)
    
    # Сохранение результатов
    final_results = clusterer.save_results(dialog_info, labels, cluster_analysis, OUTPUT_FILE)
    
    print(f"\nGMM кластеризация завершена! Результаты сохранены в {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
