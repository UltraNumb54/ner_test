import xmltodict
import json
import re
import html
from typing import List, Dict, Any
import os
import numpy as np

class XMLToInferenceConverter:
    def __init__(self, max_chunk_length: int = 1000):
        self.max_chunk_length = max_chunk_length
        
    def clean_text(self, text: str) -> str:
        """
        Очистка текста от проблемных символов и нормализация
        """
        if not text:
            return ""
            
        # Декодируем HTML entities
        text = html.unescape(text)
        
        # Заменяем проблемные кавычки
        text = text.replace('"', "'")
        
        # Удаляем управляющие символы и непечатаемые символы
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        
        # Заменяем множественные пробелы и переносы
        text = re.sub(r'\s+', ' ', text)
        
        # Удаляем начальные и конечные пробелы
        text = text.strip()
        
        return text
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """
        Разбивка длинного текста на логические части
        """
        if len(text) <= self.max_chunk_length:
            return [text]
        
        chunks = []
        current_chunk = ""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) > self.max_chunk_length and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def tokenize_text(self, text: str) -> List[str]:
        """
        Токенизация текста для русского языка (такая же как при обучении)
        """
        if not text:
            return []
        
        # Улучшенная токенизация для русского языка
        token_regex = r'[\w\u0400-\u04FF@]+|[^\w\s]|_+'
        tokens = re.findall(token_regex, text)
        
        return [token for token in tokens if token.strip()]
    
    def extract_text_from_description(self, description_element) -> str:
        """
        Извлечение текста из description с учетом CDATA
        """
        if description_element is None:
            return ""
            
        # xmltodict сохраняет CDATA как обычный текст
        if isinstance(description_element, dict):
            # Если description содержит сложную структуру
            return self.extract_text_from_dict(description_element)
        elif isinstance(description_element, str):
            return self.clean_text(description_element)
        else:
            return self.clean_text(str(description_element))
    
    def extract_text_from_dict(self, data_dict: Dict) -> str:
        """
        Рекурсивно извлекает текст из сложных словарных структур
        """
        if isinstance(data_dict, str):
            return data_dict
        
        text_parts = []
        
        # Рекурсивно обходим словарь
        for key, value in data_dict.items():
            if isinstance(value, dict):
                text_parts.append(self.extract_text_from_dict(value))
            elif isinstance(value, list):
                for item in value:
                    text_parts.append(self.extract_text_from_dict(item))
            elif value is not None:
                text_parts.append(str(value))
        
        return ' '.join(text_parts)
    
    def convert_numpy_types(self, obj):
        """
        Рекурсивно преобразует numpy типы в нативные Python типы для JSON сериализации
        """
        if isinstance(obj, dict):
            return {key: self.convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_numpy_types(item) for item in obj]
        elif isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (str, int, float, bool)) or obj is None:
            return obj
        else:
            return str(obj)
    
    def convert_xml_to_inference_format(self, xml_file_path: str, output_file: str = None) -> List[Dict[str, Any]]:
        """
        Конвертация XML в формат для инференса
        """
        try:
            # Читаем XML файл
            with open(xml_file_path, 'r', encoding='utf-8') as file:
                xml_content = file.read()
            
            # Парсим XML в словарь с помощью xmltodict
            data_dict = xmltodict.parse(xml_content)
            
        except Exception as e:
            raise ValueError(f"Ошибка парсинга XML: {e}")
        
        dataset = []
        record_count = 0
        chunk_count = 0
        
        # Обрабатываем структуру данных
        records = data_dict.get('data', {}).get('record', [])
        if not isinstance(records, list):
            records = [records]
        
        for record in records:
            record_count += 1
            
            # Извлекаем базовые поля
            record_id = record.get('id', '')
            service = record.get('service', '')
            topic = record.get('topic', '')
            
            # Обрабатываем description
            description_content = record.get('description', '')
            original_text = self.extract_text_from_description(description_content)
            
            if not original_text:
                print(f"Предупреждение: запись {record_id} не содержит текста")
                continue
            
            # Разбиваем текст на части если нужно
            text_chunks = self.split_text_into_chunks(original_text)
            
            # Обрабатываем attachments
            attachments = record.get('attachments', [])
            if not isinstance(attachments, list):
                attachments = [attachments] if attachments else []
            
            for chunk_index, chunk_text in enumerate(text_chunks):
                chunk_count += 1
                
                # Токенизируем текст (такая же токенизация как при обучении)
                tokens = self.tokenize_text(chunk_text)
                
                # Создаем запись для инференса
                dataset_entry = {
                    "id": f"{record_id}_{chunk_index}",
                    "original_text": chunk_text,
                    "tokens": tokens,
                    "ner_tags": ["O"] * len(tokens),  # Заполняем O, модель заменит на предсказания
                    "success": False,
                    "metadata": {
                        "original_id": str(record_id),
                        "service": self.clean_text(str(service)),
                        "topic": self.clean_text(str(topic)),
                        "attachments_count": len(attachments),
                        "chunk_index": chunk_index,
                        "total_chunks": len(text_chunks)
                    }
                }
                
                dataset.append(dataset_entry)
        
        print(f"Обработано записей: {record_count}")
        print(f"Создано чанков: {chunk_count}")
        print(f"Всего элементов для инференса: {len(dataset)}")
        
        # Преобразуем numpy типы в нативные Python типы
        dataset = self.convert_numpy_types(dataset)
        
        # Сохраняем в файл если указан output
        if output_file:
            self.save_to_json(dataset, output_file)
            print(f"Данные для инференса сохранены в: {output_file}")
        
        return dataset
    
    def save_to_json(self, dataset: List[Dict], output_file: str):
        """
        Сохранение датасета в JSON файл с обработкой numpy типов
        """
        # Дополнительная проверка на numpy типы
        def default_serializer(obj):
            if isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64)):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2, default=default_serializer)

# Обновленный класс TextPreprocessor для защиты NER меток
class TextPreprocessor:
    def __init__(self, 
                 remove_brackets: bool = True,
                 to_lower: bool = True,
                 remove_punctuation: bool = True,
                 remove_digits: bool = True,
                 remove_stopwords: bool = True,
                 lemmatize: bool = False,
                 protect_ner_tags: bool = True):  # Новый параметр для защиты меток
        
        self.remove_brackets = remove_brackets
        self.to_lower = to_lower
        self.remove_punctuation = remove_punctuation
        self.remove_digits = remove_digits
        self.remove_stopwords = remove_stopwords
        self.lemmatize = lemmatize
        self.protect_ner_tags = protect_ner_tags  # Защищать ли NER метки [ENTITY]
        
        # Базовый список стоп-слов для русского языка
        self.stopwords = set([
            'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 
            'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 
            'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 
            'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 
            'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 
            'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 
            'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 
            'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 
            'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 
            'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 
            'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'
        ])
        
        # Инициализация лемматизатора
        if self.lemmatize:
            self.morph = pymorphy3.MorphAnalyzer()
    
    def remove_bracket_content(self, text: str) -> Tuple[str, List[Dict]]:
        """
        Удаление всего в [], {}, <> включая сами скобки
        Но если protect_ner_tags=True, то защищаем NER метки [ENTITY]
        """
        original_text = text
        
        if self.protect_ner_tags:
            # Временная замена NER меток перед удалением скобок
            ner_pattern = r'\[([A-Z_]+)\]'
            ner_matches = list(re.finditer(ner_pattern, text))
            
            # Временные метки для защиты NER
            protected_text = text
            replacements = {}
            
            for i, match in enumerate(ner_matches):
                placeholder = f"__NER_{i}__"
                replacements[placeholder] = match.group(0)  # Сохраняем оригинальную метку
                protected_text = protected_text.replace(match.group(0), placeholder, 1)
            
            # Удаляем остальные скобки
            protected_text = re.sub(r'\[.*?\]', '', protected_text)
            protected_text = re.sub(r'\{.*?\}', '', protected_text)
            protected_text = re.sub(r'\<.*?\>', '', protected_text)
            
            # Восстанавливаем NER метки
            text = protected_text
            for placeholder, ner_tag in replacements.items():
                text = text.replace(placeholder, ner_tag)
        else:
            # Стандартное удаление скобок
            text = re.sub(r'\[.*?\]', '', text)
            text = re.sub(r'\{.*?\}', '', text)
            text = re.sub(r'\<.*?\>', '', text)
        
        # Убираем лишние пробелы после удаления
        text = re.sub(r'\s+', ' ', text).strip()
        
        replacements_list = []
        if text != original_text:
            replacements_list.append({
                'type': 'remove_brackets',
                'from': original_text,
                'to': text
            })
        
        return text, replacements_list
    
    def remove_punctuation_and_special(self, text: str) -> Tuple[str, List[Dict]]:
        """
        Замена всей пунктуации и спецсимволов на пробел
        Но если protect_ner_tags=True, то защищаем NER метки [ENTITY]
        """
        original_text = text
        
        if self.protect_ner_tags:
            # Временная замена NER меток
            ner_pattern = r'\[([A-Z_]+)\]'
            ner_matches = list(re.finditer(ner_pattern, text))
            
            protected_text = text
            replacements = {}
            
            for i, match in enumerate(ner_matches):
                placeholder = f"__NER_{i}__"
                replacements[placeholder] = match.group(0)
                protected_text = protected_text.replace(match.group(0), placeholder, 1)
            
            # Применяем стандартную обработку пунктуации
            import string
            translator = str.maketrans(string.punctuation + '«»—…', ' ' * (len(string.punctuation) + 4))
            protected_text = protected_text.translate(translator)
            
            # Восстанавливаем NER метки
            text = protected_text
            for placeholder, ner_tag in replacements.items():
                text = text.replace(placeholder, ner_tag)
        else:
            # Стандартная обработка пунктуации
            import string
            translator = str.maketrans(string.punctuation + '«»—…', ' ' * (len(string.punctuation) + 4))
            text = text.translate(translator)
        
        replacements_list = []
        if text != original_text:
            replacements_list.append({
                'type': 'remove_punctuation',
                'from': original_text,
                'to': text
            })
        
        return text, replacements_list
    
    def to_lower_case(self, text: str) -> Tuple[str, List[Dict]]:
        """
        Приведение текста к нижнему регистру
        Но если protect_ner_tags=True, то защищаем NER метки [ENTITY]
        """
        original_text = text
        
        if self.protect_ner_tags:
            # Временная замена NER меток
            ner_pattern = r'\[([A-Z_]+)\]'
            ner_matches = list(re.finditer(ner_pattern, text))
            
            protected_text = text
            replacements = {}
            
            for i, match in enumerate(ner_matches):
                placeholder = f"__NER_{i}__"
                replacements[placeholder] = match.group(0)
                protected_text = protected_text.replace(match.group(0), placeholder, 1)
            
            # Применяем нижний регистр
            protected_text = protected_text.lower()
            
            # Восстанавливаем NER метки
            text = protected_text
            for placeholder, ner_tag in replacements.items():
                text = text.replace(placeholder, ner_tag)
        else:
            text = text.lower()
        
        replacements_list = []
        if text != original_text:
            replacements_list.append({
                'type': 'to_lower',
                'from': original_text,
                'to': text
            })
        
        return text, replacements_list
    
    # Остальные методы класса TextPreprocessor остаются без изменений
    # но будут использовать protect_ner_tags там где это необходимо
    
    def remove_digits_from_text(self, text: str) -> Tuple[str, List[Dict]]:
        """
        Удаление цифр из текста (не затрагивает NER метки)
        """
        original_text = text
        
        if self.protect_ner_tags:
            # Временная замена NER меток
            ner_pattern = r'\[([A-Z_]+)\]'
            ner_matches = list(re.finditer(ner_pattern, text))
            
            protected_text = text
            replacements = {}
            
            for i, match in enumerate(ner_matches):
                placeholder = f"__NER_{i}__"
                replacements[placeholder] = match.group(0)
                protected_text = protected_text.replace(match.group(0), placeholder, 1)
            
            # Удаляем цифры
            protected_text = re.sub(r'\d+', '', protected_text)
            
            # Восстанавливаем NER метки
            text = protected_text
            for placeholder, ner_tag in replacements.items():
                text = text.replace(placeholder, ner_tag)
        else:
            text = re.sub(r'\d+', '', text)
        
        replacements_list = []
        if text != original_text:
            replacements_list.append({
                'type': 'remove_digits',
                'from': original_text,
                'to': text
            })
        
        return text, replacements_list

# Прямое указание путей в коде
if __name__ == "__main__":
    INPUT_XML_FILE = "data.xml"  # Путь к вашему XML файлу
    OUTPUT_JSON_FILE = "inference_data.json"  # Путь для сохранения JSON
    
    # Параметры конвертации
    MAX_CHUNK_LENGTH = 1000  # Максимальная длина текстового чанка
    
    # Проверяем существование входного файла
    if not os.path.exists(INPUT_XML_FILE):
        print(f"Ошибка: файл {INPUT_XML_FILE} не найден!")
        print("Убедитесь, что файл существует в указанной директории.")
        exit(1)
    
    # Создаем конвертер и выполняем конвертацию
    converter = XMLToInferenceConverter(max_chunk_length=MAX_CHUNK_LENGTH)
    
    try:
        dataset = converter.convert_xml_to_inference_format(INPUT_XML_FILE, OUTPUT_JSON_FILE)
        print("Конвертация завершена успешно!")
        
    except Exception as e:
        print(f"Ошибка при конвертации: {e}")
