import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification
)
from datasets import Dataset
import numpy as np
from sklearn.model_selection import train_test_split
import os
import evaluate

class NERModelTrainer:
    def __init__(self, model_name="bert-base-multilingual-cased", max_length=512):
        self.model_name = model_name
        self.max_length = max_length
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.label2id = {}
        self.id2label = {}
        
    def load_and_filter_data(self, json_file_path):
        """
        Загрузка и фильтрация данных из JSON файла
        """
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Фильтруем только размеченные данные
        labeled_data = [item for item in data if item.get('success', False)]
        
        print(f"Загружено {len(labeled_data)} размеченных примеров из {len(data)} всего")
        
        # Проверяем и исправляем метки
        validated_data = self.validate_and_fix_labels(labeled_data)
        
        return validated_data
    
    def validate_and_fix_labels(self, data):
        """
        Проверяет и исправляет несогласованные метки в данных
        """
        valid_data = []
        invalid_count = 0
        
        for item in data:
            tokens = item['tokens']
            ner_tags = item['ner_tags']
            
            # Проверяем корректность последовательности меток
            if self.is_valid_sequence(ner_tags):
                valid_data.append(item)
            else:
                print(f"Обнаружена некорректная последовательность в примере {item['id']}")
                print(f"Текст: {item['original_text'][:100]}...")
                print(f"Токены: {tokens}")
                print(f"Метки до исправления: {ner_tags}")
                
                # Исправляем метки (превращаем проблемные I- в O)
                fixed_tags = self.fix_inconsistent_tags(ner_tags)
                item['ner_tags'] = fixed_tags
                valid_data.append(item)
                invalid_count += 1
                print(f"Метки после исправления: {fixed_tags}")
                print("---")
        
        print(f"Исправлено {invalid_count} примеров с некорректными метками")
        return valid_data
    
    def is_valid_sequence(self, tags):
        """
        Проверяет, что последовательность меток корректна
        """
        for i, tag in enumerate(tags):
            if tag.startswith('I-'):
                # I- метка должна следовать за B- или I- той же сущности
                entity_type = tag[2:]
                if i == 0 or (tags[i-1] != f'B-{entity_type}' and tags[i-1] != f'I-{entity_type}'):
                    return False
        return True
    
    def fix_inconsistent_tags(self, tags):
        """
        Исправляет несогласованные метки
        """
        fixed_tags = tags.copy()
        
        for i, tag in enumerate(fixed_tags):
            if tag.startswith('I-'):
                entity_type = tag[2:]
                # Если I- метка стоит в начале или после другой сущности, превращаем её в O
                if i == 0 or (fixed_tags[i-1] != f'B-{entity_type}' and fixed_tags[i-1] != f'I-{entity_type}'):
                    fixed_tags[i] = 'O'
        
        return fixed_tags
    
    def extract_labels_from_data(self, data):
        """
        Извлечение всех уникальных меток из данных с дополнительной проверкой
        """
        all_labels = set()
        for item in data:
            for tag in item['ner_tags']:
                if tag != 'O':
                    all_labels.add(tag)
        
        # Сортируем метки для воспроизводимости
        sorted_labels = sorted(list(all_labels))
        
        # Создаем словари для преобразования меток
        labels = ['O'] + sorted_labels
        
        # Убеждаемся, что все метки корректны
        self.label2id = {}
        self.id2label = {}
        
        for idx, label in enumerate(labels):
            self.label2id[label] = idx
            self.id2label[idx] = label
        
        print(f"Найдено {len(labels)} уникальных меток:")
        for label in labels:
            print(f"  {label} -> {self.label2id[label]}")
        
        # Дополнительная проверка: убедимся, что все метки в данных есть в label2id
        self.validate_all_labels_in_mapping(data)
        
        return labels
    
    def validate_all_labels_in_mapping(self, data):
        """
        Проверяет, что все метки в данных есть в label2id
        """
        missing_labels = set()
        for item in data:
            for tag in item['ner_tags']:
                if tag not in self.label2id:
                    missing_labels.add(tag)
        
        if missing_labels:
            print(f"ВНИМАНИЕ: Найдены метки, отсутствующие в маппинге: {missing_labels}")
            print("Добавляем отсутствующие метки в маппинг...")
            
            # Добавляем отсутствующие метки
            for label in sorted(missing_labels):
                new_id = len(self.label2id)
                self.label2id[label] = new_id
                self.id2label[new_id] = label
                print(f"  Добавлена метка: {label} -> {new_id}")
        
        return len(missing_labels) == 0
    
    def align_labels_with_tokens(self, labels, word_ids):
        """
        Выравнивание меток с токенами после токенизации BERT
        """
        aligned_labels = []
        current_word = None
        
        for word_id in word_ids:
            if word_id is None:
                # Специальные токены ([CLS], [SEP], [PAD])
                aligned_labels.append(-100)
            elif word_id != current_word:
                # Начало нового слова
                current_word = word_id
                if word_id < len(labels):
                    aligned_labels.append(self.label2id[labels[word_id]])
                else:
                    # Если word_id выходит за границы, используем O
                    aligned_labels.append(self.label2id['O'])
            else:
                # Продолжение того же слова (подтокены)
                if word_id < len(labels):
                    label = labels[word_id]
                    # Если это продолжение сущности, используем I- префикс
                    if label.startswith('B-'):
                        # Преобразуем B- в I- для подтокенов
                        i_label = 'I-' + label[2:]
                        if i_label in self.label2id:
                            aligned_labels.append(self.label2id[i_label])
                        else:
                            # Если I- метки нет, используем O
                            aligned_labels.append(self.label2id['O'])
                    else:
                        aligned_labels.append(self.label2id[label])
                else:
                    aligned_labels.append(self.label2id['O'])
        
        return aligned_labels
    
    def tokenize_and_align_labels(self, examples):
        """
        Токенизация текста и выравнивание меток
        """
        tokenized_inputs = self.tokenizer(
            examples['tokens'],
            truncation=True,
            padding=False,
            is_split_into_words=True,
            max_length=self.max_length,
            return_tensors=None
        )
        
        all_labels = []
        for i, labels in enumerate(examples['ner_tags']):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            aligned_labels = self.align_labels_with_tokens(labels, word_ids)
            all_labels.append(aligned_labels)
        
        tokenized_inputs['labels'] = all_labels
        return tokenized_inputs
    
    def prepare_dataset(self, data):
        """
        Подготовка данных в формате для обучения
        """
        # Преобразуем в формат datasets
        formatted_data = {
            'tokens': [item['tokens'] for item in data],
            'ner_tags': [item['ner_tags'] for item in data]
        }
        
        dataset = Dataset.from_dict(formatted_data)
        tokenized_dataset = dataset.map(
            self.tokenize_and_align_labels,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        return tokenized_dataset
    
    def compute_metrics(self, eval_pred):
        """
        Исправленная функция вычисления метрик с обработкой KeyError
        """
        try:
            metric = evaluate.load("seqeval")
            
            predictions, labels = eval_pred
            
            # Убеждаемся, что predictions и labels имеют правильную форму
            if isinstance(predictions, tuple):
                predictions = predictions[0]
            
            # Преобразуем предсказания в индексы меток
            predictions = np.argmax(predictions, axis=2)
            
            # Преобразуем в строковые метки
            true_predictions = []
            true_labels = []
            
            for prediction, label in zip(predictions, labels):
                pred_sequence = []
                label_sequence = []
                
                for p, l in zip(prediction, label):
                    # Игнорируем специальные токены (label = -100)
                    if l != -100:
                        try:
                            pred_sequence.append(self.id2label[p])
                            label_sequence.append(self.id2label[l])
                        except KeyError as e:
                            print(f"KeyError: {e}")
                            print(f"p: {p}, l: {l}")
                            print(f"Доступные ключи в id2label: {list(self.id2label.keys())}")
                            # Используем 'O' для неизвестных меток
                            pred_sequence.append('O')
                            label_sequence.append('O')
                
                true_predictions.append(pred_sequence)
                true_labels.append(label_sequence)
            
            # Вычисляем метрики
            results = metric.compute(predictions=true_predictions, references=true_labels)
            
            return {
                "precision": results["overall_precision"],
                "recall": results["overall_recall"],
                "f1": results["overall_f1"],
                "accuracy": results["overall_accuracy"],
            }
            
        except Exception as e:
            print(f"Ошибка в compute_metrics: {e}")
            return {"f1": 0.0, "precision": 0.0, "recall": 0.0, "accuracy": 0.0}
    
    def debug_data_structure(self, data):
        """
        Отладочная функция для анализа структуры данных
        """
        print("=== ДЕБАГ СТРУКТУРЫ ДАННЫХ ===")
        
        # Анализируем первые 3 примера
        for i, item in enumerate(data[:3]):
            print(f"Пример {i}:")
            print(f"  ID: {item['id']}")
            print(f"  Токены: {item['tokens']}")
            print(f"  NER теги: {item['ner_tags']}")
            print(f"  Длина токенов: {len(item['tokens'])}")
            print(f"  Длина тегов: {len(item['ner_tags'])}")
            
            # Проверяем уникальные метки в примере
            unique_tags = set(item['ner_tags'])
            print(f"  Уникальные метки: {unique_tags}")
            
            # Проверяем, все ли метки есть в label2id
            missing = [tag for tag in unique_tags if tag not in self.label2id]
            if missing:
                print(f"  ОТСУТСТВУЮЩИЕ МЕТКИ: {missing}")
            print("---")
    
    def train(self, json_file_path, output_dir="./ner_model", train_size=0.8, epochs=3, batch_size=16):
        """
        Основной метод обучения модели
        """
        # Загрузка данных
        print("Загрузка данных...")
        data = self.load_and_filter_data(json_file_path)
        
        if len(data) == 0:
            raise ValueError("Нет размеченных данных для обучения!")
        
        # Извлечение меток
        print("Извлечение меток...")
        self.extract_labels_from_data(data)
        
        # Отладочная информация
        self.debug_data_structure(data)
        
        # Разделение на train/validation
        train_data, val_data = train_test_split(
            data, 
            train_size=train_size, 
            random_state=42,
            shuffle=True
        )
        
        print(f"Разделение данных: {len(train_data)} train, {len(val_data)} validation")
        
        # Проверяем, что в данных есть примеры
        if len(train_data) == 0 or len(val_data) == 0:
            raise ValueError("После разделения один из наборов данных пуст!")
        
        # Подготовка датасетов
        print("Подготовка датасетов...")
        train_dataset = self.prepare_dataset(train_data)
        val_dataset = self.prepare_dataset(val_data)
        
        # Загрузка модели
        print("Загрузка модели...")
        model = AutoModelForTokenClassification.from_pretrained(
            self.model_name,
            num_labels=len(self.label2id),
            id2label=self.id2label,
            label2id=self.label2id
        )
        
        # Коллатор данных для пакетной обработки
        data_collator = DataCollatorForTokenClassification(
            tokenizer=self.tokenizer,
            padding=True
        )
        
        # Аргументы обучения
        training_args = TrainingArguments(
            output_dir=output_dir,
            learning_rate=2e-5,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            num_train_epochs=epochs,
            weight_decay=0.01,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            logging_dir=f"{output_dir}/logs",
            logging_steps=50,
            save_total_limit=2,
            seed=42,
            data_seed=42,
            warmup_steps=500,
            remove_unused_columns=False,
        )
        
        # Тренер
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
        )
        
        # Обучение
        print("Начало обучения...")
        try:
            trainer.train()
        except Exception as e:
            print(f"Ошибка во время обучения: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        # Сохранение модели
        print("Сохранение модели...")
        trainer.save_model(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        # Сохранение информации о метках ВМЕСТЕ с моделью
        label_info = {
            'label2id': self.label2id,
            'id2label': self.id2label
        }
        with open(f"{output_dir}/label_info.json", 'w', encoding='utf-8') as f:
            json.dump(label_info, f, ensure_ascii=False, indent=2)
        
        print(f"Модель сохранена в {output_dir}")
        
        # Финальная оценка
        print("Финальная оценка...")
        try:
            eval_results = trainer.evaluate()
            print("Результаты оценки:")
            for key, value in eval_results.items():
                print(f"  {key}: {value:.4f}")
        except Exception as e:
            print(f"Ошибка при оценке: {e}")
        
        return trainer

def main():
    """
    Пример использования
    """
    # Укажите путь к вашему JSON файлу
    JSON_FILE_PATH = "merged_dataset.json"  # Замените на путь к вашему файлу
    OUTPUT_DIR = "./trained_ner_model"
    
    # Проверка существования файла
    if not os.path.exists(JSON_FILE_PATH):
        print(f"Ошибка: файл {JSON_FILE_PATH} не найден!")
        return
    
    # Создание тренера и обучение
    trainer = NERModelTrainer(
        model_name="bert-base-multilingual-cased",
        max_length=512
    )
    
    try:
        result = trainer.train(
            json_file_path=JSON_FILE_PATH,
            output_dir=OUTPUT_DIR,
            epochs=3,
            batch_size=16,
            train_size=0.8
        )
        
        if result is not None:
            print("Обучение завершено успешно!")
        else:
            print("Обучение завершилось с ошибками!")
        
    except Exception as e:
        print(f"Ошибка при обучении: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
