import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification,
    EarlyStoppingCallback
)
from datasets import Dataset
import numpy as np
from sklearn.model_selection import train_test_split, KFold
import os
import evaluate

class NERModelTrainer:
    def __init__(self, model_name="bert-base-multilingual-cased", max_length=512, local_files_only=True):
        self.model_name = model_name
        self.max_length = max_length
        self.local_files_only = local_files_only
        
        # Загрузка токенизатора локально
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            local_files_only=local_files_only
        )
        self.label2id = {}
        self.id2label = {}
        
    def load_and_filter_data(self, json_file_path):
        """
        Загрузка и фильтрация данных из JSON файла
        """
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Фильтруем только размеченные данные
        labeled_data = [item for item in data if item.get('success', False)]
        
        print(f"Загружено {len(labeled_data)} размеченных примеров из {len(data)} всего")
        
        # Проверяем и исправляем метки
        validated_data = self.validate_and_fix_labels(labeled_data)
        
        return validated_data
    
    def validate_and_fix_labels(self, data):
        """
        Проверяет и исправляет несогласованные метки в данных
        """
        valid_data = []
        invalid_count = 0
        
        for item in data:
            tokens = item['tokens']
            ner_tags = item['ner_tags']
            
            # Проверяем корректность последовательности меток
            if self.is_valid_sequence(ner_tags):
                valid_data.append(item)
            else:
                print(f"Обнаружена некорректная последовательность в примере {item['id']}")
                print(f"Текст: {item['original_text'][:100]}...")
                print(f"Токены: {tokens}")
                print(f"Метки до исправления: {ner_tags}")
                
                # Исправляем метки (превращаем проблемные I- в O)
                fixed_tags = self.fix_inconsistent_tags(ner_tags)
                item['ner_tags'] = fixed_tags
                valid_data.append(item)
                invalid_count += 1
                print(f"Метки после исправления: {fixed_tags}")
                print("---")
        
        print(f"Исправлено {invalid_count} примеров с некорректными метками")
        return valid_data
    
    def is_valid_sequence(self, tags):
        """
        Проверяет, что последовательность меток корректна
        """
        for i, tag in enumerate(tags):
            if tag.startswith('I-'):
                # I- метка должна следовать за B- или I- той же сущности
                entity_type = tag[2:]
                if i == 0 or (tags[i-1] != f'B-{entity_type}' and tags[i-1] != f'I-{entity_type}'):
                    return False
        return True
    
    def fix_inconsistent_tags(self, tags):
        """
        Исправляет несогласованные метки
        """
        fixed_tags = tags.copy()
        
        for i, tag in enumerate(fixed_tags):
            if tag.startswith('I-'):
                entity_type = tag[2:]
                # Если I- метка стоит в начале или после другой сущности, превращаем её в O
                if i == 0 or (fixed_tags[i-1] != f'B-{entity_type}' and fixed_tags[i-1] != f'I-{entity_type}'):
                    fixed_tags[i] = 'O'
        
        return fixed_tags
    
    def extract_labels_from_data(self, data):
        """
        Извлечение всех уникальных меток из данных с дополнительной проверкой
        """
        all_labels = set()
        for item in data:
            for tag in item['ner_tags']:
                if tag != 'O':
                    all_labels.add(tag)
        
        # Сортируем метки для воспроизводимости
        sorted_labels = sorted(list(all_labels))
        
        # Создаем словари для преобразования меток
        labels = ['O'] + sorted_labels
        
        # Убеждаемся, что все метки корректны
        self.label2id = {}
        self.id2label = {}
        
        for idx, label in enumerate(labels):
            self.label2id[label] = idx
            self.id2label[idx] = label
        
        print(f"Найдено {len(labels)} уникальных меток:")
        for label in labels:
            print(f"  {label} -> {self.label2id[label]}")
        
        # Дополнительная проверка: убедимся, что все метки в данных есть в label2id
        self.validate_all_labels_in_mapping(data)
        
        return labels
    
    def validate_all_labels_in_mapping(self, data):
        """
        Проверяет, что все метки в данных есть в label2id
        """
        missing_labels = set()
        for item in data:
            for tag in item['ner_tags']:
                if tag not in self.label2id:
                    missing_labels.add(tag)
        
        if missing_labels:
            print(f"ВНИМАНИЕ: Найдены метки, отсутствующие в маппинге: {missing_labels}")
            print("Добавляем отсутствующие метки в маппинг...")
            
            # Добавляем отсутствующие метки
            for label in sorted(missing_labels):
                new_id = len(self.label2id)
                self.label2id[label] = new_id
                self.id2label[new_id] = label
                print(f"  Добавлена метка: {label} -> {new_id}")
        
        return len(missing_labels) == 0
    
    def align_labels_with_tokens(self, labels, word_ids):
        """
        Выравнивание меток с токенами после токенизации BERT
        """
        aligned_labels = []
        current_word = None
        
        for word_id in word_ids:
            if word_id is None:
                # Специальные токены ([CLS], [SEP], [PAD])
                aligned_labels.append(-100)
            elif word_id != current_word:
                # Начало нового слова
                current_word = word_id
                if word_id < len(labels):
                    aligned_labels.append(self.label2id[labels[word_id]])
                else:
                    # Если word_id выходит за границы, используем O
                    aligned_labels.append(self.label2id['O'])
            else:
                # Продолжение того же слова (подтокены)
                if word_id < len(labels):
                    label = labels[word_id]
                    # Если это продолжение сущности, используем I- префикс
                    if label.startswith('B-'):
                        # Преобразуем B- в I- для подтокенов
                        i_label = 'I-' + label[2:]
                        if i_label in self.label2id:
                            aligned_labels.append(self.label2id[i_label])
                        else:
                            # Если I- метки нет, используем O
                            aligned_labels.append(self.label2id['O'])
                    else:
                        aligned_labels.append(self.label2id[label])
                else:
                    aligned_labels.append(self.label2id['O'])
        
        return aligned_labels
    
    def tokenize_and_align_labels(self, examples):
        """
        Токенизация текста и выравнивание меток
        """
        tokenized_inputs = self.tokenizer(
            examples['tokens'],
            truncation=True,
            padding=False,
            is_split_into_words=True,
            max_length=self.max_length,
            return_tensors=None
        )
        
        all_labels = []
        for i, labels in enumerate(examples['ner_tags']):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            aligned_labels = self.align_labels_with_tokens(labels, word_ids)
            all_labels.append(aligned_labels)
        
        tokenized_inputs['labels'] = all_labels
        return tokenized_inputs
    
    def prepare_dataset(self, data):
        """
        Подготовка данных в формате для обучения
        """
        # Преобразуем в формат datasets
        formatted_data = {
            'tokens': [item['tokens'] for item in data],
            'ner_tags': [item['ner_tags'] for item in data]
        }
        
        dataset = Dataset.from_dict(formatted_data)
        tokenized_dataset = dataset.map(
            self.tokenize_and_align_labels,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        return tokenized_dataset
    
    def compute_detailed_metrics(self, eval_pred):
        """
        Детализированные метрики по типам сущностей
        """
        try:
            metric = evaluate.load("seqeval")
            predictions, labels = eval_pred
            
            # Убеждаемся, что predictions и labels имеют правильную форму
            if isinstance(predictions, tuple):
                predictions = predictions[0]
            
            # Преобразуем предсказания в индексы меток
            predictions = np.argmax(predictions, axis=2)
            
            # Преобразуем в строковые метки
            true_predictions = []
            true_labels = []
            
            for prediction, label in zip(predictions, labels):
                pred_sequence = []
                label_sequence = []
                
                for p, l in zip(prediction, label):
                    # Игнорируем специальные токены (label = -100)
                    if l != -100:
                        try:
                            pred_sequence.append(self.id2label[p])
                            label_sequence.append(self.id2label[l])
                        except KeyError as e:
                            print(f"KeyError: {e}")
                            print(f"p: {p}, l: {l}")
                            print(f"Доступные ключи в id2label: {list(self.id2label.keys())}")
                            # Используем 'O' для неизвестных меток
                            pred_sequence.append('O')
                            label_sequence.append('O')
                
                true_predictions.append(pred_sequence)
                true_labels.append(label_sequence)
            
            # Вычисляем метрики
            results = metric.compute(predictions=true_predictions, references=true_labels)
            
            # Детализированные результаты
            detailed_results = {
                "precision": results["overall_precision"],
                "recall": results["overall_recall"],
                "f1": results["overall_f1"],
                "accuracy": results["overall_accuracy"],
            }
            
            # Добавляем метрики по классам
            for key, value in results.items():
                if key not in ["overall_precision", "overall_recall", "overall_f1", "overall_accuracy"]:
                    if isinstance(value, dict):
                        detailed_results[f"{key}_f1"] = value.get("f1", 0)
                        detailed_results[f"{key}_precision"] = value.get("precision", 0)
                        detailed_results[f"{key}_recall"] = value.get("recall", 0)
            
            return detailed_results
            
        except Exception as e:
            print(f"Ошибка в compute_metrics: {e}")
            return {"f1": 0.0, "precision": 0.0, "recall": 0.0, "accuracy": 0.0}
    
    def debug_data_structure(self, data):
        """
        Отладочная функция для анализа структуры данных
        """
        print("=== ДЕБАГ СТРУКТУРЫ ДАННЫХ ===")
        
        # Анализируем первые 3 примера
        for i, item in enumerate(data[:3]):
            print(f"Пример {i}:")
            print(f"  ID: {item['id']}")
            print(f"  Токены: {item['tokens']}")
            print(f"  NER теги: {item['ner_tags']}")
            print(f"  Длина токенов: {len(item['tokens'])}")
            print(f"  Длина тегов: {len(item['ner_tags'])}")
            
            # Проверяем уникальные метки в примере
            unique_tags = set(item['ner_tags'])
            print(f"  Уникальные метки: {unique_tags}")
            
            # Проверяем, все ли метки есть в label2id
            missing = [tag for tag in unique_tags if tag not in self.label2id]
            if missing:
                print(f"  ОТСУТСТВУЮЩИЕ МЕТКИ: {missing}")
            print("---")
    
    def get_optimized_training_args(self, output_dir, batch_size=8, dataset_size=700):
        """
        Оптимизированные параметры для маленького датасета
        """
        # Эмпирические правила для маленьких датасетов
        effective_batch_size = batch_size * torch.cuda.device_count() if torch.cuda.is_available() else batch_size
        steps_per_epoch = max(1, dataset_size // effective_batch_size)
        
        return TrainingArguments(
            output_dir=output_dir,
            learning_rate=3e-5,  # Чуть выше learning rate
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            num_train_epochs=15,  # Максимум
            weight_decay=0.01,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            logging_dir=f"{output_dir}/logs",
            logging_steps=max(10, steps_per_epoch // 5),  # Адаптивный logging
            save_total_limit=2,
            seed=42,
            warmup_ratio=0.1,  # Процент от общего числа шагов для warmup
            remove_unused_columns=False,
            dataloader_num_workers=0,  # Для избежания проблем на Windows
            report_to=None,  # Отключаем отчеты для полной локальности
        )
    
    def _train_single_fold(self, train_data, val_data, output_dir, epochs=10, batch_size=8):
        """
        Обучение одной модели на одном фолде
        """
        # Подготовка датасетов
        train_dataset = self.prepare_dataset(train_data)
        val_dataset = self.prepare_dataset(val_data)
        
        # Загрузка модели локально
        model = AutoModelForTokenClassification.from_pretrained(
            self.model_name,
            num_labels=len(self.label2id),
            id2label=self.id2label,
            label2id=self.label2id,
            local_files_only=self.local_files_only
        )
        
        # Коллатор данных для пакетной обработки
        data_collator = DataCollatorForTokenClassification(
            tokenizer=self.tokenizer,
            padding=True
        )
        
        # Аргументы обучения
        training_args = self.get_optimized_training_args(
            output_dir, 
            batch_size=batch_size, 
            dataset_size=len(train_data)
        )
        
        # Тренер
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_detailed_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Остановка после 3 эпох без улучшений
        )
        
        # Обучение
        print(f"Начало обучения фолда...")
        try:
            trainer.train()
            
            # Сохранение модели
            trainer.save_model(output_dir)
            self.tokenizer.save_pretrained(output_dir)
            
            # Сохранение информации о метках
            label_info = {
                'label2id': self.label2id,
                'id2label': self.id2label
            }
            with open(f"{output_dir}/label_info.json", 'w', encoding='utf-8') as f:
                json.dump(label_info, f, ensure_ascii=False, indent=2)
            
            return trainer
            
        except Exception as e:
            print(f"Ошибка во время обучения фолда: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def train(self, json_file_path, output_dir="./ner_model", train_size=0.8, epochs=10, batch_size=8):
        """
        Основной метод обучения модели с early stopping
        """
        # Загрузка данных
        print("Загрузка данных...")
        data = self.load_and_filter_data(json_file_path)
        
        if len(data) == 0:
            raise ValueError("Нет размеченных данных для обучения!")
        
        # Извлечение меток
        print("Извлечение меток...")
        self.extract_labels_from_data(data)
        
        # Отладочная информация
        self.debug_data_structure(data)
        
        # Разделение на train/validation
        train_data, val_data = train_test_split(
            data, 
            train_size=train_size, 
            random_state=42,
            shuffle=True
        )
        
        print(f"Разделение данных: {len(train_data)} train, {len(val_data)} validation")
        
        # Проверяем, что в данных есть примеры
        if len(train_data) == 0 or len(val_data) == 0:
            raise ValueError("После разделения один из наборов данных пуст!")
        
        # Обучение одной модели
        trainer = self._train_single_fold(train_data, val_data, output_dir, epochs, batch_size)
        
        if trainer:
            # Финальная оценка
            print("Финальная оценка...")
            try:
                eval_results = trainer.evaluate()
                print("Результаты оценки:")
                for key, value in eval_results.items():
                    if isinstance(value, float):
                        print(f"  {key}: {value:.4f}")
            except Exception as e:
                print(f"Ошибка при оценке: {e}")
        
        return trainer
    
    def train_with_cross_validation(self, json_file_path, output_dir="./ner_model_cv", 
                                  n_splits=3, epochs=10, batch_size=8):
        """
        Обучение с кросс-валидацией
        """
        # Загрузка данных
        print("Загрузка данных для кросс-валидации...")
        data = self.load_and_filter_data(json_file_path)
        
        if len(data) == 0:
            raise ValueError("Нет размеченных данных для обучения!")
        
        # Извлечение меток
        print("Извлечение меток...")
        self.extract_labels_from_data(data)
        
        # K-Fold кросс-валидация
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
        fold_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(data)):
            print(f"\n=== Обучение Fold {fold + 1}/{n_splits} ===")
            
            # Разделение данных
            train_data = [data[i] for i in train_idx]
            val_data = [data[i] for i in val_idx]
            
            print(f"Fold {fold + 1}: {len(train_data)} train, {len(val_data)} validation")
            
            # Обучение для этого фолда
            fold_dir = f"{output_dir}_fold_{fold+1}"
            trainer = self._train_single_fold(train_data, val_data, fold_dir, epochs, batch_size)
            
            if trainer:
                eval_results = trainer.evaluate()
                fold_f1 = eval_results.get('eval_f1', 0)
                fold_scores.append(fold_f1)
                print(f"Fold {fold+1} F1: {fold_f1:.4f}")
        
        print(f"\n=== Результаты кросс-валидации ===")
        if fold_scores:
            print(f"Средний F1: {np.mean(fold_scores):.4f} (±{np.std(fold_scores):.4f})")
            print(f"Результаты по фолдам: {fold_scores}")
        
        return fold_scores

def create_sample_dataset(output_path="sample_dataset.json"):
    """
    Создает пример датасета для тестирования (если у вас нет данных)
    """
    sample_data = [
        {
            "id": 1,
            "success": True,
            "tokens": ["Иван", "Иванов", "работает", "в", "компании", "Рога", "и", "копыта"],
            "ner_tags": ["B-PER", "I-PER", "O", "O", "O", "B-ORG", "I-ORG", "I-ORG"],
            "original_text": "Иван Иванов работает в компании Рога и копыта"
        },
        {
            "id": 2,
            "success": True,
            "tokens": ["Мария", "Петрова", "живет", "в", "Москве"],
            "ner_tags": ["B-PER", "I-PER", "O", "O", "B-LOC"],
            "original_text": "Мария Петрова живет в Москве"
        },
        {
            "id": 3,
            "success": True,
            "tokens": ["Компания", "Apple", "выпустила", "новый", "iPhone"],
            "ner_tags": ["O", "B-ORG", "O", "O", "O"],
            "original_text": "Компания Apple выпустила новый iPhone"
        }
    ]
    
    # Создаем больше примеров для реалистичного датасета
    for i in range(4, 701):
        sample_data.append({
            "id": i,
            "success": True,
            "tokens": ["Пример", "текста", "с", "разными", "сущностями"],
            "ner_tags": ["O", "O", "O", "O", "O"],
            "original_text": f"Пример текста {i} с разными сущностями"
        })
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    
    print(f"Создан пример датасета с {len(sample_data)} записями в {output_path}")
    return output_path

def main():
    """
    Пример использования с полной локальностью
    """
    # Укажите путь к вашему JSON файлу
    JSON_FILE_PATH = "merged_dataset.json"
    
    # Если файла нет, создаем пример датасета
    if not os.path.exists(JSON_FILE_PATH):
        print(f"Файл {JSON_FILE_PATH} не найден. Создаю пример датасета...")
        JSON_FILE_PATH = create_sample_dataset()
    
    OUTPUT_DIR = "./trained_ner_model"
    
    # Создание тренера с полной локальностью
    trainer = NERModelTrainer(
        model_name="bert-base-multilingual-cased",  # Должна быть скачана заранее
        max_length=256,  # Уменьшаем для ускорения
        local_files_only=True  # Важно: только локальные файлы
    )
    
    try:
        # Вариант 1: Обычное обучение с early stopping
        print("=== ЗАПУСК ОБЫЧНОГО ОБУЧЕНИЯ ===")
        result = trainer.train(
            json_file_path=JSON_FILE_PATH,
            output_dir=OUTPUT_DIR,
            epochs=15,  # Максимум, но остановится раньше благодаря early stopping
            batch_size=8,
            train_size=0.8
        )
        
        # Вариант 2: Кросс-валидация (раскомментируйте для использования)
        # print("\n=== ЗАПУСК КРОСС-ВАЛИДАЦИИ ===")
        # cv_scores = trainer.train_with_cross_validation(
        #     json_file_path=JSON_FILE_PATH,
        #     output_dir=OUTPUT_DIR,
        #     n_splits=3,  # 3-fold CV для 700 примеров
        #     epochs=10,
        #     batch_size=8
        # )
        
        if result is not None:
            print("Обучение завершено успешно!")
            
            # Тестирование сохраненной модели
            print("\n=== ТЕСТИРОВАНИЕ МОДЕЛИ ===")
            test_sentence = "Иван Иванов работает в компании Apple в Москве"
            test_tokens = test_sentence.split()
            
            # Загрузка сохраненной модели для тестирования
            from transformers import pipeline
            
            try:
                ner_pipeline = pipeline(
                    "ner",
                    model=OUTPUT_DIR,
                    tokenizer=OUTPUT_DIR,
                    aggregation_strategy="simple"
                )
                
                results = ner_pipeline(test_sentence)
                print(f"Тестовое предложение: {test_sentence}")
                print("Результаты NER:")
                for entity in results:
                    print(f"  {entity['word']} -> {entity['entity_group']} (score: {entity['score']:.3f})")
                    
            except Exception as e:
                print(f"Ошибка при тестировании: {e}")
                
        else:
            print("Обучение завершилось с ошибками!")
        
    except Exception as e:
        print(f"Ошибка при обучении: {e}")
        import traceback
        traceback.print_exc()

def download_model_locally(model_name="bert-base-multilingual-cased", local_dir="./local_model"):
    """
    Функция для предварительной загрузки модели (выполнить один раз)
    """
    if not os.path.exists(local_dir):
        print(f"Загрузка модели {model_name} в {local_dir}...")
        
        # Скачиваем токенизатор
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.save_pretrained(local_dir)
        
        # Скачиваем модель
        model = AutoModelForTokenClassification.from_pretrained(model_name)
        model.save_pretrained(local_dir)
        
        print(f"Модель сохранена в {local_dir}")
    else:
        print(f"Модель уже существует в {local_dir}")
    
    return local_dir

if __name__ == "__main__":
    # Перед первым запуском выполните эту функцию для загрузки модели:
    # LOCAL_MODEL_PATH = download_model_locally()
    
    # Затем используйте локальную модель:
    # trainer = NERModelTrainer(model_name=LOCAL_MODEL_PATH, local_files_only=True)
    
    main()
