import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification
)
from datasets import Dataset
import numpy as np
from sklearn.model_selection import train_test_split
import os
import evaluate

class NERModelTrainer:
    def __init__(self, model_name="bert-base-multilingual-cased", max_length=512):
        self.model_name = model_name
        self.max_length = max_length
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.label2id = {}
        self.id2label = {}
        
    def load_and_filter_data(self, json_file_path):
        """
        Загрузка и фильтрация данных из JSON файла
        """
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Фильтруем только размеченные данные
        labeled_data = [item for item in data if item.get('success', False)]
        
        print(f"Загружено {len(labeled_data)} размеченных примеров из {len(data)} всего")
        
        # Проверяем и исправляем метки
        validated_data = self.validate_and_fix_labels(labeled_data)
        
        return validated_data
    
    def validate_and_fix_labels(self, data):
        """
        Проверяет и исправляет несогласованные метки в данных
        """
        valid_data = []
        invalid_count = 0
        
        for item in data:
            tokens = item['tokens']
            ner_tags = item['ner_tags']
            
            # Проверяем корректность последовательности меток
            if self.is_valid_sequence(ner_tags):
                valid_data.append(item)
            else:
                print(f"Обнаружена некорректная последовательность в примере {item['id']}")
                print(f"Текст: {item['original_text'][:100]}...")
                print(f"Токены: {tokens}")
                print(f"Метки: {ner_tags}")
                
                # Исправляем метки (превращаем проблемные I- в O)
                fixed_tags = self.fix_inconsistent_tags(ner_tags)
                item['ner_tags'] = fixed_tags
                valid_data.append(item)
                invalid_count += 1
        
        print(f"Исправлено {invalid_count} примеров с некорректными метками")
        return valid_data
    
    def is_valid_sequence(self, tags):
        """
        Проверяет, что последовательность меток корректна
        """
        for i, tag in enumerate(tags):
            if tag.startswith('I-'):
                # I- метка должна следовать за B- или I- той же сущности
                entity_type = tag[2:]
                if i == 0 or (tags[i-1] != f'B-{entity_type}' and tags[i-1] != f'I-{entity_type}'):
                    return False
        return True
    
    def fix_inconsistent_tags(self, tags):
        """
        Исправляет несогласованные метки
        """
        fixed_tags = tags.copy()
        
        for i, tag in enumerate(fixed_tags):
            if tag.startswith('I-'):
                entity_type = tag[2:]
                # Если I- метка стоит в начале или после другой сущности, превращаем её в O
                if i == 0 or (fixed_tags[i-1] != f'B-{entity_type}' and fixed_tags[i-1] != f'I-{entity_type}'):
                    fixed_tags[i] = 'O'
        
        return fixed_tags
    
    def extract_labels_from_data(self, data):
        """
        Извлечение всех уникальных меток из данных
        """
        all_labels = set()
        for item in data:
            for tag in item['ner_tags']:
                if tag != 'O':
                    all_labels.add(tag)
        
        # Сортируем метки для воспроизводимости
        sorted_labels = sorted(list(all_labels))
        
        # Создаем словари для преобразования меток
        labels = ['O'] + sorted_labels
        self.label2id = {label: idx for idx, label in enumerate(labels)}
        self.id2label = {idx: label for idx, label in enumerate(labels)}
        
        print(f"Найдено {len(labels)} уникальных меток:")
        for label in labels:
            print(f"  {label} -> {self.label2id[label]}")
        
        return labels
    
    def align_labels_with_tokens(self, labels, word_ids):
        """
        Выравнивание меток с токенами после токенизации BERT
        """
        aligned_labels = []
        current_word = None
        
        for word_id in word_ids:
            if word_id is None:
                # Специальные токены ([CLS], [SEP], [PAD])
                aligned_labels.append(-100)
            elif word_id != current_word:
                # Начало нового слова
                current_word = word_id
                aligned_labels.append(self.label2id[labels[word_id]])
            else:
                # Продолжение того же слова (подтокены)
                label = labels[word_id]
                # Если это продолжение сущности, используем I- префикс
                if label.startswith('B-'):
                    # Преобразуем B- в I- для подтокенов
                    aligned_labels.append(self.label2id['I-' + label[2:]])
                else:
                    aligned_labels.append(self.label2id[label])
        
        return aligned_labels
    
    def tokenize_and_align_labels(self, examples):
        """
        Токенизация текста и выравнивание меток
        """
        tokenized_inputs = self.tokenizer(
            examples['tokens'],
            truncation=True,
            padding=False,
            is_split_into_words=True,
            max_length=self.max_length,
            return_tensors=None
        )
        
        all_labels = []
        for i, labels in enumerate(examples['ner_tags']):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            aligned_labels = self.align_labels_with_tokens(labels, word_ids)
            all_labels.append(aligned_labels)
        
        tokenized_inputs['labels'] = all_labels
        return tokenized_inputs
    
    def prepare_dataset(self, data):
        """
        Подготовка данных в формате для обучения
        """
        # Преобразуем в формат datasets
        formatted_data = {
            'tokens': [item['tokens'] for item in data],
            'ner_tags': [item['ner_tags'] for item in data]
        }
        
        dataset = Dataset.from_dict(formatted_data)
        tokenized_dataset = dataset.map(
            self.tokenize_and_align_labels,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        return tokenized_dataset
    
    def compute_metrics(self, eval_pred):
        """
        Исправленная функция вычисления метрик
        """
        metric = evaluate.load("seqeval")
        
        predictions, labels = eval_pred
        
        # Убеждаемся, что predictions и labels имеют правильную форму
        if isinstance(predictions, tuple):
            predictions = predictions[0]
        
        # Преобразуем предсказания в индексы меток
        predictions = np.argmax(predictions, axis=2)
        
        # Преобразуем в строковые метки
        true_predictions = []
        true_labels = []
        
        for prediction, label in zip(predictions, labels):
            pred_sequence = []
            label_sequence = []
            
            for p, l in zip(prediction, label):
                # Игнорируем специальные токены (label = -100)
                if l != -100:
                    pred_sequence.append(self.id2label[str(p)])
                    label_sequence.append(self.id2label[str(l)])
            
            true_predictions.append(pred_sequence)
            true_labels.append(label_sequence)
        
        # Вычисляем метрики
        results = metric.compute(predictions=true_predictions, references=true_labels)
        
        return {
            "precision": results["overall_precision"],
            "recall": results["overall_recall"],
            "f1": results["overall_f1"],
            "accuracy": results["overall_accuracy"],
        }
    
    def train(self, json_file_path, output_dir="./ner_model", train_size=0.8, epochs=3, batch_size=16):
        """
        Основной метод обучения модели
        """
        # Загрузка данных
        print("Загрузка данных...")
        data = self.load_and_filter_data(json_file_path)
        
        if len(data) == 0:
            raise ValueError("Нет размеченных данных для обучения!")
        
        # Извлечение меток
        print("Извлечение меток...")
        self.extract_labels_from_data(data)
        
        # Разделение на train/validation
        train_data, val_data = train_test_split(
            data, 
            train_size=train_size, 
            random_state=42,
            shuffle=True
        )
        
        print(f"Разделение данных: {len(train_data)} train, {len(val_data)} validation")
        
        # Проверяем, что в данных есть примеры
        if len(train_data) == 0 or len(val_data) == 0:
            raise ValueError("После разделения один из наборов данных пуст!")
        
        # Подготовка датасетов
        print("Подготовка датасетов...")
        train_dataset = self.prepare_dataset(train_data)
        val_dataset = self.prepare_dataset(val_data)
        
        # Загрузка модели
        print("Загрузка модели...")
        model = AutoModelForTokenClassification.from_pretrained(
            self.model_name,
            num_labels=len(self.label2id),
            id2label=self.id2label,
            label2id=self.label2id
        )
        
        # Коллатор данных для пакетной обработки
        data_collator = DataCollatorForTokenClassification(
            tokenizer=self.tokenizer,
            padding=True
        )
        
        # Аргументы обучения
        training_args = TrainingArguments(
            output_dir=output_dir,
            learning_rate=2e-5,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            num_train_epochs=epochs,
            weight_decay=0.01,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            logging_dir=f"{output_dir}/logs",
            logging_steps=50,
            save_total_limit=2,
            seed=42,
            data_seed=42,
            warmup_steps=500,
            remove_unused_columns=False,  # Важно для корректной работы
        )
        
        # Тренер
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
        )
        
        # Обучение
        print("Начало обучения...")
        trainer.train()
        
        # Сохранение модели
        print("Сохранение модели...")
        trainer.save_model(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        # Сохранение информации о метках ВМЕСТЕ с моделью
        label_info = {
            'label2id': self.label2id,
            'id2label': self.id2label
        }
        with open(f"{output_dir}/label_info.json", 'w', encoding='utf-8') as f:
            json.dump(label_info, f, ensure_ascii=False, indent=2)
        
        print(f"Модель сохранена в {output_dir}")
        
        # Финальная оценка
        print("Финальная оценка...")
        eval_results = trainer.evaluate()
        print("Результаты оценки:")
        for key, value in eval_results.items():
            print(f"  {key}: {value:.4f}")
        
        return trainer

def main():
    """
    Пример использования
    """
    # Укажите путь к вашему JSON файлу
    JSON_FILE_PATH = "merged_dataset.json"  # Замените на путь к вашему файлу
    OUTPUT_DIR = "./trained_ner_model"
    
    # Проверка существования файла
    if not os.path.exists(JSON_FILE_PATH):
        print(f"Ошибка: файл {JSON_FILE_PATH} не найден!")
        return
    
    # Создание тренера и обучение
    trainer = NERModelTrainer(
        model_name="bert-base-multilingual-cased",
        max_length=512
    )
    
    try:
        trainer.train(
            json_file_path=JSON_FILE_PATH,
            output_dir=OUTPUT_DIR,
            epochs=3,
            batch_size=16,
            train_size=0.8
        )
        print("Обучение завершено успешно!")
        
    except Exception as e:
        print(f"Ошибка при обучении: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
