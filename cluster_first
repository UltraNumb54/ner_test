# clustering_first_messages.py
import json
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from tqdm import tqdm
import os

class FirstMessageClustering:
    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):
        """Инициализация модели для создания эмбеддингов"""
        print(f"Загрузка модели {model_name}")
        self.model = SentenceTransformer(model_name)
        print("Модель загружена успешно")
    
    def load_data(self, clustering_file, analysis_file):
        """Загрузка данных для кластеризации по первым сообщениям"""
        print("Загрузка данных...")
        
        # Загрузка данных для кластеризации
        with open(clustering_file, 'r', encoding='utf-8') as f:
            clustering_data = json.load(f)
        
        # Загрузка полных диалогов для анализа
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        
        # Создаем словарь для быстрого доступа к полным диалогам
        analysis_dict = {item['dialog_id']: item for item in analysis_data}
        
        texts = []
        dialog_info = []
        
        for item in clustering_data:
            dialog_id = item.get('dialog_id')
            first_message = item.get('first_message_replaced', '')
            full_text = item.get('full_text_replaced', '')
            
            if first_message and len(first_message.strip()) > 10:  # Минимальная длина
                texts.append(first_message.strip())
                
                # Получаем полный диалог с ролями
                full_dialog = analysis_dict.get(dialog_id, {})
                
                dialog_info.append({
                    'dialog_id': dialog_id,
                    'first_message': first_message,
                    'full_text': full_text,
                    'full_dialog': full_dialog,  # Полный диалог с ролями
                    'original_topic': full_dialog.get('topic', 'unknown'),
                    'source': full_dialog.get('source', 'unknown')
                })
        
        print(f"Загружено {len(texts)} первых сообщений для кластеризации")
        return texts, dialog_info
    
    def create_embeddings(self, texts, batch_size=32):
        """Создание векторных представлений текстов"""
        print("Создание эмбеддингов...")
        
        embeddings = self.model.encode(
            texts, 
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        print(f"Создано эмбеддингов: {embeddings.shape}")
        return embeddings
    
    def find_optimal_clusters(self, embeddings, max_k=15):
        """Поиск оптимального количества кластеров методом локтя и силуэтного анализа"""
        print("Поиск оптимального количества кластеров...")
        
        if len(embeddings) < 3:
            print("Недостаточно данных для анализа кластеров")
            return 2
        
        wcss = []  # Within-Cluster Sum of Square
        silhouette_scores = []
        k_range = range(2, min(max_k + 1, len(embeddings)))
        
        for k in tqdm(k_range, desc="Анализ кластеров"):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(embeddings)
            wcss.append(kmeans.inertia_)
            
            if len(set(labels)) > 1:
                score = silhouette_score(embeddings, labels)
                silhouette_scores.append(score)
            else:
                silhouette_scores.append(0)
        
        # Метод локтя: находим "локоть" на графике WCSS
        differences = []
        for i in range(1, len(wcss)):
            differences.append(wcss[i-1] - wcss[i])
        
        # Находим точку, где разница начинает выравниваться
        elbow_k = 2
        if differences:
            avg_reduction = np.mean(differences)
            for i, diff in enumerate(differences):
                if diff < avg_reduction * 0.5:  # Когда снижение становится меньше половины среднего
                    elbow_k = i + 2
                    break
        
        # Силуэтный анализ
        if silhouette_scores:
            silhouette_k = k_range[np.argmax(silhouette_scores)]
        else:
            silhouette_k = 2
        
        # Выбираем оптимальное k
        optimal_k = max(elbow_k, silhouette_k)
        
        print(f"Оптимальное количество кластеров:")
        print(f"  Метод локтя: {elbow_k}")
        print(f"  Силуэтный анализ: {silhouette_k}")
        print(f"  Выбрано: {optimal_k}")
        
        # Визуализация выбора k
        self._plot_optimal_clusters(k_range, wcss, silhouette_scores, elbow_k, silhouette_k)
        
        return optimal_k
    
    def _plot_optimal_clusters(self, k_range, wcss, silhouette_scores, elbow_k, silhouette_k):
        """Визуализация выбора оптимального количества кластеров"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # График метода локтя
        ax1.plot(k_range, wcss, 'bo-')
        ax1.axvline(x=elbow_k, color='red', linestyle='--', alpha=0.7, label=f'Локоть: k={elbow_k}')
        ax1.set_xlabel('Количество кластеров')
        ax1.set_ylabel('WCSS (Within-Cluster Sum of Square)')
        ax1.set_title('Метод локтя')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # График силуэтного анализа
        ax2.plot(k_range, silhouette_scores, 'go-')
        ax2.axvline(x=silhouette_k, color='red', linestyle='--', alpha=0.7, label=f'Оптимум: k={silhouette_k}')
        ax2.set_xlabel('Количество кластеров')
        ax2.set_ylabel('Silhouette Score')
        ax2.set_title('Силуэтный анализ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('clustering_optimal_k_first_messages.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("График выбора оптимального k сохранен: clustering_optimal_k_first_messages.png")
    
    def perform_clustering(self, embeddings, n_clusters=None):
        """Выполнение кластеризации K-means"""
        if n_clusters is None:
            n_clusters = min(10, max(2, len(embeddings) // 10))
        
        print(f"Выполнение K-means кластеризации с {n_clusters} кластерами...")
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings)
        
        # Оценка качества
        if len(set(labels)) > 1:
            score = silhouette_score(embeddings, labels)
            print(f"Silhouette Score: {score:.4f}")
        else:
            print("Создан только один кластер")
        
        return labels, kmeans
    
    def extract_keywords(self, texts, n_keywords=10):
        """Извлечение ключевых слов из текстов"""
        all_text = " ".join(texts).lower()
        
        # Токенизация и очистка
        words = re.findall(r'\b[а-яё]{3,}\b', all_text)
        
        # Список стоп-слов (можно расширить)
        stop_words = {
            'этот', 'такой', 'какой', 'который', 'очень', 'много', 'можно', 
            'нужно', 'должен', 'хочу', 'хотеть', 'хотят', 'свой', 'мочь',
            'будет', 'есть', 'быть', 'сказать', 'говорить', 'пожалуйста',
            'здравствуйте', 'спасибо', 'проблема', 'вопрос', 'помощь'
        }
        
        # Фильтрация стоп-слов и подсчет частот
        word_counts = Counter(words)
        filtered_words = {word: count for word, count in word_counts.items() 
                         if word not in stop_words and count > 1}
        
        return Counter(filtered_words).most_common(n_keywords)
    
    def analyze_clusters(self, texts, labels, dialog_info):
        """Анализ кластеров и извлечение ключевых характеристик"""
        print("Анализ кластеров...")
        
        cluster_analysis = {}
        
        for cluster_id in set(labels):
            cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]
            cluster_texts = [texts[i] for i in cluster_indices]
            cluster_dialogs = [dialog_info[i] for i in cluster_indices]
            
            # Ключевые слова
            keywords = self.extract_keywords(cluster_texts)
            
            # Анализ первых сообщений
            sample_messages = cluster_texts[:5]
            
            # Анализ частотности меток
            metka_stats = self._analyze_metkas(cluster_texts)
            
            cluster_analysis[cluster_id] = {
                'size': len(cluster_texts),
                'keywords': keywords,
                'sample_messages': sample_messages,
                'metka_stats': metka_stats,
                'avg_message_length': np.mean([len(text) for text in cluster_texts])
            }
        
        return cluster_analysis
    
    def _analyze_metkas(self, texts):
        """Анализ частотности меток в кластере"""
        metkas = ['[ФИО]', '[Телефон]', '[Email]', '[Дата]', '[Сумма]', 
                 '[Организация]', '[Адрес]', '[Показания]', '[Номер лицевого счёта]']
        metka_counts = {metka: 0 for metka in metkas}
        
        for text in texts:
            for metka in metkas:
                if metka in text:
                    metka_counts[metka] += 1
        
        return {k: v for k, v in metka_counts.items() if v > 0}
    
    def visualize_clusters(self, embeddings, labels, cluster_analysis):
        """Визуализация кластеров с помощью PCA"""
        print("Визуализация кластеров...")
        
        # PCA для визуализации
        pca = PCA(n_components=2, random_state=42)
        embeddings_2d = pca.fit_transform(embeddings)
        
        # Создаем DataFrame для удобства построения графиков
        df = pd.DataFrame({
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1],
            'cluster': labels
        })
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
        
        # 1. Точечная диаграмма кластеров
        scatter = ax1.scatter(df['x'], df['y'], c=df['cluster'], cmap='tab10', alpha=0.7, s=50)
        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        ax1.set_title('Визуализация кластеров (PCA)')
        ax1.legend(*scatter.legend_elements(), title="Кластеры")
        ax1.grid(True, alpha=0.3)
        
        # 2. Размеры кластеров
        cluster_sizes = {k: v['size'] for k, v in cluster_analysis.items()}
        clusters = list(cluster_sizes.keys())
        sizes = list(cluster_sizes.values())
        
        bars = ax2.bar(clusters, sizes, color=plt.cm.tab10(range(len(clusters))))
        ax2.set_xlabel('Номер кластера')
        ax2.set_ylabel('Количество диалогов')
        ax2.set_title('Распределение диалогов по кластерам')
        
        # Добавляем подписи на столбцах
        for bar, size in zip(bars, sizes):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{size}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('clustering_visualization_first_messages.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("Графики кластеризации сохранены: clustering_visualization_first_messages.png")
        
        # Выводим информацию о PCA
        print(f"PCA объясненная дисперсия: PC1={pca.explained_variance_ratio_[0]:.2%}, PC2={pca.explained_variance_ratio_[1]:.2%}")
    
    def save_results(self, dialog_info, labels, cluster_analysis, output_file):
        """Сохранение результатов кластеризации"""
        print("Сохранение результатов...")
        
        results = []
        for i, (item, label) in enumerate(zip(dialog_info, labels)):
            results.append({
                'dialog_id': item['dialog_id'],
                'cluster_id': int(label),
                'cluster_size': cluster_analysis[label]['size'],
                'first_message_replaced': item['first_message'],
                'full_dialog': item['full_dialog'],  # Полный диалог с ролями
                'keywords': [word for word, count in cluster_analysis[label]['keywords'][:5]],
                'original_topic': item.get('original_topic', 'unknown'),
                'source': item.get('source', 'unknown')
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self._print_statistics(results, cluster_analysis)
        
        return results
    
    def _print_statistics(self, results, cluster_analysis):
        """Вывод статистики кластеризации"""
        print("\n" + "="*60)
        print("СТАТИСТИКА КЛАСТЕРИЗАЦИИ ПО ПЕРВЫМ СООБЩЕНИЯМ")
        print("="*60)
        print(f"Всего диалогов: {len(results)}")
        print(f"Количество кластеров: {len(cluster_analysis)}")
        
        print("\nРаспределение по кластерам:")
        for cluster_id, analysis in sorted(cluster_analysis.items()):
            print(f"  Кластер {cluster_id}: {analysis['size']} диалогов")
            print(f"    Ключевые слова: {[word for word, count in analysis['keywords'][:3]]}")
            if analysis['metka_stats']:
                print(f"    Частые метки: {analysis['metka_stats']}")
            print(f"    Примеры сообщений:")
            for i, msg in enumerate(analysis['sample_messages'][:2]):
                print(f"      {i+1}. {msg[:100]}..." if len(msg) > 100 else f"      {i+1}. {msg}")

def main():
    """Основная функция кластеризации по первым сообщениям"""
    
    # Конфигурация
    CLUSTERING_FILE = "clustering_data.json"
    ANALYSIS_FILE = "analysis_data_replaced.json"  # Упрощенная версия с заменами
    OUTPUT_FILE = "clustering_results_first_messages.json"
    NUM_CLUSTERS = None  # Автоопределение
    
    print("=" * 60)
    print("КЛАСТЕРИЗАЦИЯ ПО ПЕРВЫМ СООБЩЕНИЯМ")
    print("=" * 60)
    
    # Инициализация кластеризатора
    clusterer = FirstMessageClustering()
    
    # Загрузка данных
    texts, dialog_info = clusterer.load_data(CLUSTERING_FILE, ANALYSIS_FILE)
    
    if len(texts) < 3:
        print("Недостаточно данных для кластеризации (нужно минимум 3 текста)")
        return
    
    # Создание эмбеддингов
    embeddings = clusterer.create_embeddings(texts)
    
    # Определение количества кластеров
    if NUM_CLUSTERS is None:
        NUM_CLUSTERS = clusterer.find_optimal_clusters(embeddings)
    
    # Кластеризация
    labels, kmeans = clusterer.perform_clustering(embeddings, NUM_CLUSTERS)
    
    # Анализ кластеров
    cluster_analysis = clusterer.analyze_clusters(texts, labels, dialog_info)
    
    # Визуализация
    clusterer.visualize_clusters(embeddings, labels, cluster_analysis)
    
    # Сохранение результатов
    results = clusterer.save_results(dialog_info, labels, cluster_analysis, OUTPUT_FILE)
    
    print(f"\nКластеризация завершена! Результаты сохранены в {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
